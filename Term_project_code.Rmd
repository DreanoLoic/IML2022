---
title: "Term Project - Group 80"
author: "Sari Ropponen, Outi Boman, Loic Dreano"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
##Description of data

The training data npf_train.csv and testing data npf_test_hidden.csv were downloaded. The data includes 104 features. The number of observations in training data is 464 and 965 in testing data.

```{r, echo=FALSE}
library(tibble)
npf <- read.csv("npf_train.csv")
npf_hidden <- read.csv("npf_test_hidden.csv")

df1=data_frame(" "=c("Measurements","Variables"))
df1$npf_train = dim(npf)
df1$npf_test = dim(npf_hidden)

knitr::kable(df1, digits = 3)
# str(npf)
```

The features include a lot of daily measurements taken in Hyytiälä forestry field station. Some of the features like temperature T and CO2 are measured in different heights - the height indicated in the name of the feature, like T84.mean is the mean temperature at 8.4 meters above the mast base. 

## Preprocessing data

The summary of first 10 features in training data is:

```{r, echo=FALSE}
summary(npf[,1:10])
# check that there is no variable with sd = 0 and remove them (only work for numercical or factor variable)
# npf.var = npf[, -which(apply(npf, 2, sd) == 0)]
# same goal but based on the length of the result of the unique function()
rownames(npf) <- npf[,"date"]
npf <- npf[, -c(1, 2)]
npf.var = npf[,-which(apply(npf, 2, function(x) length(unique(x))) == 1)] 
```

The column "date" was set to be the row names. Columns "id", "date" and "partlybad" were removed from the data. Because the value of the logical variable "partlybad" is FALSE for all the observations, it doesn't give any information.

A qualitative variable "class2" is added to the training data. It gets either value "event" or "nonevent" according to "class4". Variable "class4" indicates the type of the event if it has happened, values "Ia", "Ib" or "II", or "nonevent" if no event has happened during the day. 

```{r, echo=FALSE}
npf.var$class2 <- factor("event", levels = c("nonevent", "event"))
npf.var$class2[npf$class4 =="nonevent"] <- "nonevent"
# reodering of npf.var to have the classes column at the beginning (easier to remove them after)
npf.var = npf.var[,c(1,102,2:101)]
```

The task is to build a classifier which divides observations into two classes: "event" or "nonevent".

## Data exploration

Because the data includes same measurements at different heights it is expected that there are correlation between variables. First, the correlations between different mean values are investigated:

```{r, echo=FALSE,fig.align="center"}
library(corrplot)
cm <- cor(npf.var[, endsWith(colnames(npf.var), ".mean")])
# Remove ".mean" from variable names
colnames(cm) <- rownames(cm) <- sapply(colnames(cm), function(s) gsub(".mean","",s))
corrplot(cm, order = "FPC", tl.cex=0.5, tl.col= "black")

```
The same picture for different standard deviation values:
```{r, echo=FALSE,fig.align="center"}
# the correlation for the standard deviation variable 
cm_sd <- cor(npf.var[, endsWith(colnames(npf.var), ".std")])
colnames(cm_sd) <- rownames(cm_sd) <- sapply(colnames(cm_sd), function(s) gsub(".std","",s))
corrplot(cm_sd, order = "FPC", tl.cex=0.5, tl.col= "black")
```


The variables which have an absolute correlation greater than 0.8 with some other variable is removed from the data. The correlations after cleaning are:
---- I GET A WARNING MESSAGE "Error: object 'no' not found"
```{r,clean based on correlation, echo=FALSE}
# Function that remove variables if the correlation is greater than s (have to adapt)
elimcor_sansY<-function(X,s=0.80){
  #X Matrix of variable to cluster 
  #s limit value of correlation 
  correl=cor(X)
  stop=F
  possetap=1:ncol(X)
  groupes=as.list(1:ncol(X))
  
  while (stop==F)
  {
    ##Grouping of var based on |corr|>0.95
    gplist<-list(NULL)
    possglob=1:ncol(correl)
    for (i in 1:(ncol(correl)))
    {
      poss=possglob[-i]
      gplist[[i]]=c(i,poss[abs(correl[i,poss])>s])
    }
    ## sort the groups from bigger to smaller
    gplisteff=unlist(lapply(gplist,length))
    if (any(gplisteff>1))
    {
      gplistfin=gplist[gplisteff>1]
      gplistuniq=unlist(gplist[gplisteff==1])
      gpsel=NULL
      ## pick randomly one variable from each group to keep
      for (i in 1:length(gplistfin))
      {
        selloc=min(gplistfin[[i]])
        gploc=groupes[[possetap[selloc]]]
        for (j in 1:length(gplistfin[[i]]))
        {
          gploc=c(gploc,groupes[[possetap[gplistfin[[i]][j]]]])				    }
        groupes[[possetap[selloc]]]=unique(gploc)
        gpsel=c(gpsel,selloc)
      }
      possetap=possetap[c(gplistuniq,unique(gpsel))]
      correl=cor(X[,possetap])
    }
    else stop=T	
  }
  return(list(possetap=possetap,groupes=groupes))
}

npf.corr= npf.var[,-c(1,2)]

# ind.corr.90 = elimcor_sansY(npf.corr,0.90)$possetap
ind.corr.80 = elimcor_sansY(npf.corr,0.80)$possetap

# npf.clean = npf.corr[ind.corr.90] # 34 variables
npf.clean = npf.corr[ind.corr.80] # 26 variables 
no

cm.2 <- cor(npf.clean[, endsWith(colnames(npf.clean), ".mean")])
colnames(cm.2) <- rownames(cm.2) <- sapply(colnames(cm.2), function(s) gsub(".mean","",s))
corrplot(cm.2, method = 'circle', type = 'upper', insig='blank',
         addCoef.col ='black', order = 'FPC', diag=FALSE, 
         tl.cex = 0.5, tl.col = "black",number.cex = 0.5)
# corrplot(cm.2, order = "FPC", tl.cex=0.5, tl.col= "black")

df2=data_frame(" "=c("Measurements","Variables"))
df2$npf_train = dim(npf.clean)
df2$npf_test = dim(npf_hidden)

knitr::kable(df2, digits = 3)
```

##Performance measures
To compare different classifiers two measures are used: accuracy and perplexity. Accuracy is the proportion of the observations that has been classified correctly. Perplexity a rescaled variant of log-likelihood. If perplexity = 1 the classifier predicts always the probability of an observation to an actual class. Perplexity = 2 corresponds to coin flipping.

For Random Forest perplexity is not calculated but instead ROC curves are looked at.
---- Did we agree on this? :)

Observation is classified as "event" if the estimated posterior probability is more than 0.5. Otherwise the observation is classified as "nonevent".

Performance measures are calculated using validation method: the training data is randomly divided into 2 equally large data sets, training data to fit the model and validation data to estimate the accuracy and perplexity. Both data sets have 232 observations. 

10-fold Cross-Validation is also used to the original training data (with 464 observations) to estimate performance measures in testing data.

The alternative response "class4" is removed from the data so that it doesn't disturb the fitting.

```{r, echo=FALSE}
# which training set ? 
npf.all.clean = cbind(class2=npf.var$class2,npf.clean) # data without correlated for class2
# npf = cbind(class2=npf.var$class4,npf.clean) # data without correlated for class4
# npf = npf.var[,-1] # Raw data for class2
# npf = npf.var[,-2] # Raw data for class4

set.seed(42)
train_id <- sample(1:nrow(npf), 232)
npf_train <- npf.all.clean[train_id,]
npf_valid <- npf.all.clean[-train_id,]

#to calculate mean, class2 needs to be converted into numeric values
class2_num_train <- c(ifelse(npf_train$class2=="event",1,0))
class2_num <- c(ifelse(npf_valid$class2=="event",1,0))
class2_num_all <- c(ifelse(npf.all.clean$class2=="event",1,0))

## accuracy if we know the probabilities of ones
accuracy <- function(p, y) mean(ifelse(p >= 0.5, 1, 0) == y)
## perplexity if we know the probabilities of ones
perplexity <- function(p, y) exp(-mean(log(ifelse(y == "event", p, 1 - p))))

#initialization of result table
npf_results <- data.frame(matrix(ncol = 7))
colnames(npf_results) = c("Model", "Train Accuracy", "Validation Accuracy","CV Accuracy", "Train Perplexity", "Validation Perplexity", "CV Perplexity")
npf_results <- na.omit(npf_results)
```

##Investigation of features with Lasso, Decision Trees and PCA
To find the most important variables logistic regression with Lasso, decision tree ("basic"), random forest and Principal Component Analysis (PCA) are used. The accuracy and perplexity are also calculated.

1) Logistic regression with Lasso, lambda selected by Cross-Validation
--- this code is updated to use npf.all.clean data in CV
```{r, echo=FALSE}
library(glmnet)
set.seed(42)
x <- model.matrix(class2 ~., npf_train)[,-1]
y <- npf_train$class2
x.test <- model.matrix(class2 ~., npf_valid)[,-1]

cv.lambda <- cv.glmnet(x, y, family = "binomial", alpha = 1)$lambda.min
m.lassoCV <- glmnet(x, y, family = "binomial", alpha = 1, lambda = cv.lambda)

#Train
phat_LGLCV_train <- predict(m.lassoCV, newx = x, type = "response")
#Test
phat_LGLCV <- predict(m.lassoCV, newx = x.test, type = "response")

#Cross-Validation
split <- c(rep_len(1:10, length.out = nrow(npf.all.clean)))
phat_LGLCV_CV <- c()
for (i in 1:10){
  x <- model.matrix(class2 ~., npf.all.clean[split!=i,])[,-1]
  y <- npf.all.clean$class2[split!=i]
  x.test <- model.matrix(class2 ~., npf.all.clean[split==i,])[,-1]
  mod <- glmnet(x, y, family = "binomial", alpha = 1, lambda = cv.lambda)
  phat_LGLCV_CV[split==i]<- predict(mod, newx = x.test, type = "response")
}

#Performance measures into the table
npf_results[nrow(npf_results)+1,] <- c("Log reg Lasso CV", 
                                       round(accuracy(phat_LGLCV_train, class2_num_train),3), 
                                       round(accuracy(phat_LGLCV, class2_num),3), 
                                       round(accuracy(phat_LGLCV_CV, class2_num_all),3), 
                                       round(perplexity(phat_LGLCV_train, npf_train$class2),3), 
                                       round(perplexity(phat_LGLCV, npf_valid$class2),3), 
                                       round(perplexity(phat_LGLCV_CV, npf.all.clean$class2),3))
```

The selected lambda is
```{r, echo=FALSE}
cv.lambda
```

and the coefficients are

```{r, echo=FALSE}
coef(m.lassoCV)
```

2) A "normal" decision tree selects the following variables with the misclassification as follows

```{r, echo=FALSE}
library(tree)
set.seed(42)
tree.npf <- tree(class2~., npf_train)
summary(tree.npf)
```

```{r, echo=FALSE}
plot(tree.npf)
text(tree.npf, pretty = 0, cex=0.6)
```
Accuracy of the tree can be calculated from the confusion matrix. Predicted classes vs. actual classes for validation data:

```{r, echo=FALSE}
tree.pred <- predict(tree.npf, npf_valid, type = "class")
table(tree.pred, npf_valid$class2)
```
Predicted classes vs. actual classes for training data:
--- this can actually be seen from the results above (summary(tree.npf)) - do we want to leave this confusion matrix away?

```{r, echo=FALSE}
tree.pred_train <- predict(tree.npf, npf_train, type = "class")
table(tree.pred_train, npf_train$class2)
```
The accuracies are respectively
```{r, echo=FALSE}
acc_tree <- (113+107)/232
acc_tree_train <- (104+94)/232
npf_results[nrow(npf_results)+1,] <- c("Tree", round(acc_tree_train,3), round(acc_tree,3), "","","","")
round(acc_tree,3)
round(acc_tree_train,3)
```

3) Random Forest with 10 variables (square root of the number of features) used in each run

Confusion matrix for training data
```{r, echo=FALSE}
library(randomForest)
set.seed(42)
rf.npf <- randomForest(class2~., data = npf_train, mtry = 10, importance = TRUE)
rf.pred_train <- predict(rf.npf, newdata = npf_train)
table(rf.pred_train, npf_train$class2)
```

Confusion matrix for validation
```{r, echo=FALSE}
rf.pred <- predict(rf.npf, newdata = npf_valid)
table(rf.pred, npf_valid$class2)
```

```{r, echo=FALSE}
acc_tree_train <- (115+117)/232
acc_tree <- (100+98)/232
npf_results[nrow(npf_results)+1,] <- c("Random Forest", round(acc_tree_train, 3), round(acc_tree,3), "", "", "","")
acc_tree_train
acc_tree
```
The importance of the variables are:

```{r, echo=FALSE}
varImpPlot(rf.npf, cex=0.6, main = "Random Forest")
```

4) PCA
PCA is used to the original training data (npf_train.csv with 464 observations) together with the original testing data (npf_hidden.csv) where the variables are centered to have zero mean and scaled to have standard deviation one. The responses variables are removed from the original training data because we are using unsupervised learning method. The first two principal components are:

```{r, eval=FALSE,echo=FALSE}
library(ggfortify)
npf_pca <- rbind(npf[,-c(1:2)], npf_hidden[,-c(1:4)])
#the old data that was used in pca: npf_pca <- npf[,-c(1,102)]
pca.npf <- prcomp(npf_pca, scale = TRUE )
#if we get to know class4 in npf_hidden, we can use colouring according to class4
#autoplot(pca.npf,data=npf_pca,colour="class4",loadings=TRUE,loadings.label=TRUE)
autoplot(pca.npf,data=npf_pca,loadings=TRUE,loadings.label=TRUE)
```

---Can we conclude from the picture above if there are any significant outliers?

The proportion of variance explained (PVE) by each principal component and the cumulative PVE is shown in the figures below:
```{r, echo=FALSE,eval=FALSE}
par(mfrow=c(1,2))
pca.npf.var <- pca.npf$sdev^2
pve <- pca.npf.var/sum(pca.npf.var)
plot(pve, xlab = "Principal Component", ylab = "Proportion of variance Explained", ylim =c(0,1), type ="b")
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative Proportion of variance Explained", ylim =c(0,1), type ="b")

```

```{r, eval=FALSE,echo=FALSE}
#The loadings of 5 first principal components are
#pca.npf$rotation[,1:5]
```

PCA is also done for the data where variables with high correleation to other variables are removed (cleaned data). The first two principal components are:

```{r, eval=FALSE,echo=FALSE}
columns <- colnames(npf.clean)
npf_hidden_clean <- npf_hidden[,colnames(npf_hidden) %in% columns]
npf_pca_clean <- rbind(npf.all.clean[,-1], npf_hidden_clean)
pca.npf.clean <- prcomp(npf_pca_clean, scale = TRUE )
#if we get to know class4 in npf_hidden, we can use colouring according to class4 (or class2)
#autoplot(pca.npf,data=npf_pca_clean,colour="class4",loadings=TRUE,loadings.label=TRUE)
autoplot(pca.npf.clean,data=npf_pca_clean,loadings=TRUE,loadings.label=TRUE)
```

The proportion of variance explained (PVE) by each principal component and the cumulative PVE is shown in the figures below:

```{r, echo=FALSE,eval=FALSE}
par(mfrow=c(1,2))
pca.npf.var <- pca.npf.clean$sdev^2
pve <- pca.npf.var/sum(pca.npf.var)
plot(pve, xlab = "Principal Component", ylab = "Proportion of variance Explained, cleaned data", ylim =c(0,1), type ="b")
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative Proportion of variance Explained, cleaned data", ylim =c(0,1), type ="b")

```

According to the results above at least 15 first principal components should be used to explaina bout 90 % of the variance.
--- If you can conclude something else from the results, you can add some text :)

The performance of the classifiers investigated so far are:

```{r, echo=FALSE,eval=FALSE}
npf_results
```

##Conclusions and feature selection
--- !!! This whole part can be removed if it's not needed. The text is not updated after cleaning the data so at least the conclusions should be updated. The models in the next session is set to use cleaned data instead of the selection done here. !!!

The accuracy is best for Random Forest, but also Logistic Regression with Lasso where lambda is selected using CV ("log reg CV") performs very well.
---- Should we investigate diagnostic plots for log reg? Should be give weight to well performed classifiers when selecting the variables?

All models use the following variables 
- RHIRGA: mean is more important than std according to tree and RF
- H2O mean
- O3 mean (Logistic Regression with Lasso, lambda = CV ("log reg CV") gives small value to std)
- CO2: tree and RF uses only std, log regs give value to mean too
- T std
- SWS.mean 

The following variables are used in all other models but RF
- NO.std
- Pamb0.mean or .std

The following variables are used in all other models but "normal" tree
- CS.mean (.std only in log reg CV)

These variables are used in some models
- UV_B.std is used only on log reg CV 
- UV_A.mean is used only in RF
- RGlob.mean and RGlob.std (RF, mean also in log reg 0.1)
- PTG.std (tree, RF)
- NET.mean
- RPAR.mean
- SO2

Because the measures in different heights/levels are highly correlated, we select just one of them.
---- which ones? Should we put more weight to the selections in random forest and log reg CV?
---- Should be produce boxplots, histograms and/or scatterplots to selected variables

We select the following variables to be used in models:
---- selected for testing the same variables than in Log Reg Lasso CV but only once if there are multiple values with different heights ------------
```{r, echo=FALSE}
selection <- c("class2", "CO2168.std", "CO242.mean", "H2O42.mean", "NO168.std", "O3504.mean", "O3168.std", "Pamb0.std", "RHIRGA504.mean", "RHIRGA672.std", "SO2168.std", "SWS.mean", "T672.std", "UV_B.std", "CS.mean", "CS.std")
  #c("class2", "CO2168.mean","CO2168.std", "Glob.mean", "H2O168.mean", "H2O168.std", "NO168.std", "O3168.mean", "Pamb0.mean", "Pamb0.std", "PTG.std", "RGlob.mean", "RHIRGA168.mean", "RHIRGA168.std", "SWS.mean", "T168.std", "CS.mean", "CS.std")
npf_train_trimmed <- npf_train[, colnames(npf_train) %in% selection]
npf_valid_trimmed <- npf_valid[, colnames(npf_train) %in% selection]
npf_trimmed <- npf[, colnames(npf) %in% selection]
selection
```

Let's check if there are any correlation left:

```{r, echo=FALSE}
library(corrplot)
cm <- cor(npf_trimmed[,-16])
colnames(cm) <- rownames(cm) <- sapply(colnames(cm), function(s) gsub(".mean","",s))
corrplot(cm, order = "FPC", tl.cex=0.5, tl.col= "black")
```


##Model selection
--- CV function from Problem 2 is below but I have not used it since both NB and logistic regression needs some modifications to functions...

```{r, echo=FALSE}
#Cross-Validation Predictions
#1) split n items into k fold of roughly equal size
kpart <- function(n,k){rep_len(1:k, lenght.out=n)}
#2) find cross-validation predictions
cv <- function(
    formula,
    data,
    model = lm,
    n = nrow(data),
    k = min(n, 10),
    split = kpart(n,k),
    train = function(data){model(formula, data = data)},
    pred = function(model, data){predict(model, newdata = data)}){
  phat <- NULL
  for (i in 1:k){
    mod <- train(data[split !=i, ])
    if (is.null(phat)){
      phat <- pred(mod, data)
    } else {
      phat[split==i]<- pred(mod, data[split==i,])
    }
  }
  phat
}

```

The models tested to the selected data in addition to decision tree, random forest and logistic regression with Lasso are

1) Dummy: 
---- write a description of the method and the code itself...

2) Naive Bayes
```{r, echo=FALSE}
library(e1071)
 
phat_NB <- function(npf_train,
npf_valid,
model = function(data) naiveBayes(class2 ~ ., data, laplace = 1),
pred = function(model, x) predict(model, x, type = "raw")[, 2]) {
  m <- model(npf_train)
  pred(m, npf_valid)
}

#Train
phat_NB_train <- phat_NB(npf_train, npf_train)
#Test
phat_NB <- phat_NB(npf_train, npf_valid)

#Cross-Validation
split <- c(rep_len(1:10, length.out = nrow(npf.all.clean)))
phat_NBCV <- c()
for (i in 1:10){
  mod <- naiveBayes(class2 ~ ., npf.all.clean[split !=i, ], laplace = 1)
  phat_NBCV[split==i]<- predict(mod, npf.all.clean[split==i,], type = "raw")[,2]
}

npf_results[nrow(npf_results)+1,] <- c("Naive Bayes", 
                                       round(accuracy(phat_NB_train, class2_num_train),3), 
                                       round(accuracy(phat_NB, class2_num),3), 
                                       round(accuracy(phat_NBCV, class2_num_all),3), 
                                       round(perplexity(phat_NB_train, npf_train$class2),3), 
                                       round(perplexity(phat_NB, npf_valid$class2),3), 
                                       round(perplexity(phat_NBCV, npf.all.clean$class2),3))

```
Accuracy of Naive Bayes when using the first 15 principal components to reduce the dimensionality of the data. The highest accuracy is marked with red.

```{r }
#perfoming PCA to the whole, cleaned data (npf_train.csv and npf_hidden.csv cleaned from correlated variables) 
pca_class2 <- prcomp(npf_pca_clean, scale = TRUE)

#calculation of accuracy on data with reduced dimensionality, reduction done by using PCs from 1 to 15
acc_NB <- c()

for (i in 1:15){
  df_train <- data.frame(pca_class2$x[train_id,1:max(1,i),drop=FALSE], class2 = npf_train$class2)
  df_valid <- data.frame(pca_class2$x[-c(train_id, 465:1429),1:max(1,i),drop=FALSE])
  
  NB <- naiveBayes(class2 ~ ., df_train, laplace = 1)
  phat <- predict(NB, df_valid, type = "raw")[, 2]
  acc_NB <- append(acc_NB, accuracy(phat, class2_num))
}

plot(c(1,15),range(acc_NB),type="n",xlab="# of principal components",ylab="accuracy")
lines(1:15, acc_NB,type="b")
i <- which.max(acc_NB); points(i,acc_NB[i],pch=16,col="red")
legend("bottomright",c("accuracy"),lty=c("solid"),pch=c(1))
```

The highest accuracy is received when using 5 first PCs.
--- Cross-Validation is not yet updated to use PCs...

```{r, echo=FALSE}
#selected number of PC
PC <- c(5)
# accuracy and perplexity of the selected PC is added to the result table
df_train <- data.frame(pca_class2$x[train_id,1:PC,drop=FALSE], class2 = npf_train$class2)
df_valid <- data.frame(pca_class2$x[-c(train_id, 465:1429),1:PC,drop=FALSE])
  
NB <- naiveBayes(class2 ~ ., df_train, laplace = 1)

#Train
phat_NB_train <- predict(NB, df_train, type = "raw")[, 2]

#Validation
phat_NB <- predict(NB, df_valid, type = "raw")[, 2]

#Cross-Validation
#split <- c(rep_len(1:10, length.out = nrow(npf.all.clean)))
#phat_NBCV <- c()
#for (i in 1:10){
#  mod <- naiveBayes(class2 ~ ., npf.all.clean[split !=i, ], laplace = 1)
#  phat_NBCV[split==i]<- predict(mod, npf.all.clean[split==i,], type = "raw")[,2]
#}

npf_results[nrow(npf_results)+1,] <- c("Naive Bayes with PCA", 
                                       round(accuracy(phat_NB_train, class2_num_train),3), 
                                       round(accuracy(phat_NB, class2_num),3), 
                                       "",
                                       #round(accuracy(phat_NBCV, class2_num_all),3), 
                                       round(perplexity(phat_NB_train, npf_train$class2),3), 
                                       round(perplexity(phat_NB, npf_valid$class2),3), 
                                       "")
                                       #round(perplexity(phat_NBCV, npf.all.clean$class2),3))

```

3) Logistic regression
---- with or without interactions?
```{r, echo=FALSE}
library(glmnet)

phat_LG <- function(npf_train_trimmed,
npf_valid_trimmed,
model = function(data) glm(class2 ~ ., data, family = "binomial"),
pred = function(model, x) predict(model, newdata = x, type = "response")) {
  m <- model(npf_train_trimmed)
  pred(m, npf_valid_trimmed)
}

#Train
phat_LG_train <- phat_LG(npf_train_trimmed, npf_train_trimmed)
#Test
phat_LG <- phat_LG(npf_train_trimmed, npf_valid_trimmed)

#Cross-Validation
split <- c(rep_len(1:10, length.out = nrow(npf_trimmed)))
phat_LGCV <- c()
for (i in 1:10){
  mod <- glm(class2 ~ ., npf_trimmed[split !=i, ], family = "binomial")
  phat_LGCV[split==i]<- predict(mod, newdata = npf_trimmed[split==i,], type = "response")
}

npf_results[nrow(npf_results)+1,] <- c("Logreg, selected data", 
                                       round(accuracy(phat_LG_train, class2_num_train),3), 
                                       round(accuracy(phat_LG, class2_num),3), 
                                       round(accuracy(phat_LGCV, class2_num_all),3), 
                                       round(perplexity(phat_LG_train, npf_train$class2),3), 
                                       round(perplexity(phat_LG, npf_valid$class2),3), 
                                       round(perplexity(phat_LGCV, npf$class2),3))
```

4) k-NN
K nearest neighbour is tried with different values of k: 1, 5, 10, 15, 20 and 50. Accuracy on validation set for each of them are respectively
```{r, echo=FALSE}
library(class)
set.seed(42)

knn.acc <- c()

train.X <- scale(npf_train_trimmed[,-16])
valid.X <- scale(npf_valid_trimmed[,-16])
train.Y <- npf_train_trimmed$class2
valid.Y <- npf_valid_trimmed$class2
j=0

for (i in c(1, 5, 10, 15, 20, 50)){
  j=j+1
  knn.Y <- knn(train.X, valid.X, train.Y, k=i)
  knn.Y_num <- ifelse(knn.Y=="event",1,0)
  knn.acc[j] <- round(mean(ifelse(knn.Y_num==class2_num,1,0)),3)
}
knn.acc
```
When k = 15, the accuracy is highest so 15-NN and 1-NN (for comparison) is added

```{r, echo=FALSE}
library(class)
set.seed(42)

train.X <- scale(npf_train_trimmed[,-16])
valid.X <- scale(npf_valid_trimmed[,-16])
train.Y <- npf_train_trimmed$class2
valid.Y <- npf_valid_trimmed$class2

#Train
knn.Y_train <- knn(train.X, train.X, train.Y, k=15)
knn.Y_train_num <- ifelse(knn.Y_train=="event",1,0)
knn.acc_train <- round(mean(ifelse(knn.Y_train_num==class2_num_train,1,0)),3)
#Validation
knn.Y <- knn(train.X, valid.X, train.Y, k=15)
knn.Y_num <- ifelse(knn.Y=="event",1,0)
knn.acc_valid <- round(mean(ifelse(knn.Y_num==class2_num,1,0)),3)

#Cross-Validation predictions of the class
knn.Y_CV <- c()
for (i in 1:10){
  train.X <- scale(npf_trimmed[split!=i,-16])
  valid.X <- scale(npf_trimmed[split==i,-16])
  train.Y <- npf_trimmed$class2[split!=i]
  valid.Y <- npf_trimmed$class2[split==i]

  knn.Y_CV[split==i] <- knn(train.X, valid.X, train.Y, k=15)
}

knn.Y_CV_num <- ifelse(knn.Y_CV=="event",1,0)
knn.acc_CV <- round(mean(ifelse(knn.Y_CV_num==class2_num_all,1,0)),3)

npf_results[nrow(npf_results)+1,] <- c("15-NN selected data", 
                                       knn.acc_train, 
                                       knn.acc_valid,
                                       knn.acc_CV,
                                       "","","")
```


```{r, echo=FALSE}
library(class)
set.seed(42)

train.X <- scale(npf_train_trimmed[,-16])
valid.X <- scale(npf_valid_trimmed[,-16])
train.Y <- npf_train_trimmed$class2
valid.Y <- npf_valid_trimmed$class2

#Train
knn.Y_train <- knn(train.X, train.X, train.Y, k=1)
knn.Y_train_num <- ifelse(knn.Y_train=="event",1,0)
knn.acc_train <- round(mean(ifelse(knn.Y_train_num==class2_num_train,1,0)),3)
#Validation
knn.Y <- knn(train.X, valid.X, train.Y, k=1)
knn.Y_num <- ifelse(knn.Y=="event",1,0)
knn.acc_valid <- round(mean(ifelse(knn.Y_num==class2_num,1,0)),3)

#Cross-Validation predictions of the class
knn.Y_CV <- c()
for (i in 1:10){
  train.X <- scale(npf_trimmed[split!=i,-16])
  valid.X <- scale(npf_trimmed[split==i,-16])
  train.Y <- npf_trimmed$class2[split!=i]
  valid.Y <- npf_trimmed$class2[split==i]

  knn.Y_CV[split==i] <- knn(train.X, valid.X, train.Y, k=1)
}

knn.Y_CV_num <- ifelse(knn.Y_CV=="event",1,0)
knn.acc_CV <- round(mean(ifelse(knn.Y_CV_num==class2_num_all,1,0)),3)

npf_results[nrow(npf_results)+1,] <- c("1-NN selected data", 
                                       knn.acc_train, 
                                       knn.acc_valid,
                                       knn.acc_CV,
                                       "","","")
```

5) SVM with radial basis kernel
----This model was mentioned in lectures in relation to npf data example so it would be nice to add...


Performance measures for the methods are:
```{r, echo=FALSE}
npf_results
```


##Some own testing...

Just testing, if perplexity should be calculated in each iteration in CV and take an average of the 10 results. Test is done to logistic regression on selected data. CV Perplexity in the result table is 1.274 and now the result is

```{r, echo=FALSE}

split <- c(rep_len(1:10, length.out = nrow(npf_trimmed)))

perplexity_testi <- c()

for (i in 1:10){
  phat_testi <- c()
  mod <- glm(class2 ~ ., npf_trimmed[split !=i, ], family = "binomial")
  phat_testi<- predict(mod, newdata = npf_trimmed[split==i,], type = "response")
  perplexity_testi[i] <- perplexity(phat_testi, npf_trimmed$class2[split==i])
}

sum(perplexity_testi)/10

```



End
