---
title: "Term Project - Group 80"
author: "Sari Ropponen, Outi Boman, Loic Dreano"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE,warning=FALSE}
library(knitr)
library(ggplot2)
library(tibble)
library(class)
library(glmnet)
library(e1071)
library(ggfortify)
library(randomForest)
library(tree)
library(glmnet)
library(corrplot)
opts_chunk$set(echo = TRUE)
```

## Description of data

The training data npf_train.csv and testing data npf_test_hidden.csv were downloaded. The data includes 104 features. The number of observations in training data is 464 and 965 in testing data.

```{r, echo=FALSE,message=F,warning=FALSE}
npf <- read.csv("npf_train.csv")
npf_hidden <- read.csv("npf_test_hidden.csv")

df1=data_frame(" "=c("Measurements","Variables"))
df1$npf_train = dim(npf)
df1$npf_test = dim(npf_hidden)

kable(df1, digits = 3, caption = "Summary of the dataset")
# str(npf)
```

The features include a lot of daily measurements taken in Hyytiälä forestry field station. Some of the features like temperature T and CO2 are measured in different heights. The height is indicated in the name of the feature, for example, T84.mean is the mean temperature at 8.4 meters above the mast base. 

## Preprocessing data

The summary of the first four features in training data is:
--- I changed this to be the 4 first because only they are needed to do the next steps of data cleaning. 

```{r, message=FALSE,echo=FALSE}
summary(npf[,1:4])
# check that there is no variable with sd = 0 and remove them (only work for numercical or factor variable)
# npf.var = npf[, -which(apply(npf, 2, sd) == 0)]
# same goal but based on the length of the result of the unique function()
rownames(npf) <- npf[,"date"]
npf <- npf[, -c(1, 2)]
npf.var = npf[,-which(apply(npf, 2, function(x) length(unique(x))) == 1)] 
```

The column "date" was set to be the row names. Columns "id", "date" and "partlybad" were removed from the data. Because the value of the logical variable "partlybad" is FALSE for all the observations, it doesn't give any information.

A qualitative variable "class2" is added to the training data. It gets either value "event" or "nonevent" according to "class4". Variable "class4" indicates the type of the event if it has happened, values "Ia", "Ib" or "II", or "nonevent" if no event has happened during the day. 

```{r, message=FALSE, echo=FALSE}
npf.var$class2 <- factor("event", levels = c("nonevent", "event"))
npf.var$class2[npf$class4 =="nonevent"] <- "nonevent"
# reodering of npf.var to have the classes column at the beginning (easier to remove them after)
npf.var = npf.var[,c(1,102,2:101)]
```

The task is to build a binary classifier which predicts if an NPF event will happen or not during the day according to the observed measurements.

## Data exploration

Because the data includes same measurements at different heights it is expected that there are correlation between variables. Correlations between different mean values are shown in the matrix below:

```{r, message=FALSE, echo=FALSE,fig.align="center"}

cm <- cor(npf.var[, endsWith(colnames(npf.var), ".mean")])
# Remove ".mean" from variable names
colnames(cm) <- rownames(cm) <- sapply(colnames(cm), function(s) gsub(".mean","",s))
corrplot(cm, order = "FPC", tl.cex=0.5, tl.col= "black",
          title="Correlation matrix of mean variables", 
     mar=c(0,0,1,0))

```
The same picture for different standard deviation values is
```{r, message=FALSE, echo=FALSE,fig.align="center"}
# the correlation for the standard deviation variable 
cm_sd <- cor(npf.var[, endsWith(colnames(npf.var), ".std")])
colnames(cm_sd) <- rownames(cm_sd) <- sapply(colnames(cm_sd), function(s) gsub(".std","",s))
corrplot(cm_sd, order = "FPC", tl.cex=0.5, tl.col= "black",
                   title="Correlation matrix of std variables", 
     mar=c(0,0,1,0))
```

As we can see, there are a lot of correlated variables which can cause problems when using machine learning methods. 
--- I think we should explain more deeply what kind of problems...

The variables which have an absolute correlation greater than 0.8 with some other variable is removed from the data. 
--- Loic, can you write above a better description of what is done (I mean, how the code selects which one are left in the data and which one are removed?)

The correlations for mean and standard deviations after cleaning the data are shown in the pictures below as well as a table of variables left in the data (table 2). Table 3 shows that the data includes only 26 variables.
--- should we put the means and std in the same correlation matrix to see if there are correlation between them and to save space?

```{r, clean based on correlation,message=FALSE, echo=FALSE}
# Function that remove variables if the correlation is greater than s (have to adapt)
elimcor_sansY<-function(X,s=0.80){
  #X Matrix of variable to cluster 
  #s limit value of correlation 
  correl=cor(X)
  stop=F
  possetap=1:ncol(X)
  groupes=as.list(1:ncol(X))
  
  while (stop==F)
  {
    ##Grouping of var based on |corr|>0.95
    gplist<-list(NULL)
    possglob=1:ncol(correl)
    for (i in 1:(ncol(correl)))
    {
      poss=possglob[-i]
      gplist[[i]]=c(i,poss[abs(correl[i,poss])>s])
    }
    ## sort the groups from bigger to smaller
    gplisteff=unlist(lapply(gplist,length))
    if (any(gplisteff>1))
    {
      gplistfin=gplist[gplisteff>1]
      gplistuniq=unlist(gplist[gplisteff==1])
      gpsel=NULL
      ## pick randomly one variable from each group to keep
      for (i in 1:length(gplistfin))
      {
        selloc=min(gplistfin[[i]])
        gploc=groupes[[possetap[selloc]]]
        for (j in 1:length(gplistfin[[i]]))
        {
          gploc=c(gploc,groupes[[possetap[gplistfin[[i]][j]]]])				    }
        groupes[[possetap[selloc]]]=unique(gploc)
        gpsel=c(gpsel,selloc)
      }
      possetap=possetap[c(gplistuniq,unique(gpsel))]
      correl=cor(X[,possetap])
    }
    else stop=T	
  }
  return(list(possetap=possetap,groupes=groupes))
}

npf.corr= npf.var[,-c(1,2)]

# ind.corr.90 = elimcor_sansY(npf.corr,0.90)$possetap
ind.corr.80 = elimcor_sansY(npf.corr,0.80)$possetap
var80 = colnames(npf.corr)[ind.corr.80]
# npf.clean = npf.corr[ind.corr.90] # 34 variables

npf.clean = npf.corr[var80] # 26 variables 
npf.test.clean = npf_hidden[var80]

cm.2 <- cor(npf.clean[, endsWith(colnames(npf.clean), ".mean")])
colnames(cm.2) <- rownames(cm.2) <- sapply(colnames(cm.2), function(s) gsub(".mean","",s))
corrplot(cm.2, method = 'circle', type = 'upper', insig='blank',
         addCoef.col ='black', order = 'FPC', diag=FALSE, 
         tl.cex = 0.5, tl.col = "black",number.cex = 0.5,
         title="Correlation matrix of mean variables for the clean dataset", 
         mar=c(0,0,1,0))
# corrplot(cm.2, order = "FPC", tl.cex=0.5, tl.col= "black")
cm.2_sd <- cor(npf.clean[, endsWith(colnames(npf.clean), ".std")])
colnames(cm.2_sd) <- rownames(cm.2_sd) <- sapply(colnames(cm.2_sd), function(s) gsub(".std","",s))
corrplot(cm.2_sd, method = 'circle', type = 'upper', insig='blank',
         addCoef.col ='black', order = 'FPC', diag=FALSE, 
         tl.cex = 0.5, tl.col = "black",number.cex = 0.5,
         title="Correlation matrix of std variables for the clean dataset", 
         mar=c(0,0,1,0))

df2=data_frame(" "=c("Measurements","Variables"))
df2$npf_train = dim(npf.clean)
df2$npf_test = dim(npf.test.clean)

df3 = data.frame(var80[1:7],var80[8:14],var80[15:21],c(var80[21:26],""))
colnames(df3)= rep("",4)

kable(df3,caption = "List of kept variables")

kable(df2, digits = 3,caption = "Summary of clean dataset")


```
--- I think we should put here some plot or graphical summary of the variables left after cleaning to get an overview of the values (for example to justify scaling in PCA).
--- Is there any good "summary picture" we could use? I tried histograms but there are too many of them...
```{r, message=FALSE, echo=FALSE}
par(mfrow=c(3,3))
for (i in (2:ncol(npf_train))){
  hist(npf_train[,i])
  #boxplot(pf_train[,i] ~class2, npf_train)
}

```


## Performance measures

To compare different classifiers two measures are used: accuracy and perplexity. Accuracy is the proportion of the observations that has been classified correctly. Perplexity is a rescaled variant of log-likelihood. If perplexity equals to 1 the classifier predicts always the probability of an observation to an actual class. Perplexity of 2 corresponds to coin flipping.

For Random Forest and decision tree perplexity is not calculated but instead ROC curves are looked at.
---- Did we agree on this? :) ROC should still be added to code below

If the method predicts a probability of an event, observation is classified as "event" if the estimated posterior probability is more than 0.5. Otherwise the observation is classified as "nonevent".

Performance measures are calculated using validation method and 10-fold Cross-Validation. For validation method the training data is randomly divided into 2 equally large data sets, training data to fit the model and validation data to estimate the accuracy and perplexity.

```{r, message=FALSE, echo=FALSE}
# which training set ? 
npf.all.clean = cbind(class2=npf.var$class2,npf.clean) # data without correlated for class2
# npf = cbind(class2=npf.var$class4,npf.clean) # data without correlated for class4
# npf.all.full = npf.var[,-1] # Raw data for class2
# npf = npf.var[,-2] # Raw data for class4

set.seed(42)
train_id <- sample(1:nrow(npf), 232)
npf_train <- npf.all.clean[train_id,]
npf_valid <- npf.all.clean[-train_id,]

#to calculate mean, class2 needs to be converted into numeric values
class2_num_train <- c(ifelse(npf_train$class2=="event",1,0))
class2_num <- c(ifelse(npf_valid$class2=="event",1,0))
class2_num_all <- c(ifelse(npf.all.clean$class2=="event",1,0))

## accuracy if we know the probabilities of ones
accuracy <- function(p, y) mean(ifelse(p >= 0.5, 1, 0) == y)
## perplexity if we know the probabilities of ones
perplexity <- function(p, y) exp(-mean(log(ifelse(y == "event", p, 1 - p))))

#initialization of result table
npf_results <- data.frame(matrix(ncol = 7))
colnames(npf_results) = c("Model", "Train Accuracy", "Validation Accuracy","CV Accuracy", "Train Perplexity", "Validation Perplexity", "CV Perplexity")
npf_results <- na.omit(npf_results)
```

## Investigation of features

To find the most important variables logistic regression with Lasso, decision tree ("basic"), random forest and Principal Component Analysis (PCA) are used to cleaned data. The accuracy of the approaches are also calculated as well as perplexity of logistic regression with Lasso.

1) Logistic regression with Lasso, lambda selected by Cross-Validation:

Logistic regression is a discriminant classifier which assumes that the log odds is linear in variables. When combined with Lasso a subset selection of variables are done by adding so called penalty term to residual sum of squares which is minimized in parameter estimation process. The bigger the estimated coefficients of the variables are the bigger the penalty. The penalty term forces some of the coefficients to zero. The amount of penalty depends on a coefficient called lambda. The value of lambda is selected by cross-validation so that the value of the test error is minimized. The lambda is

```{r, message=FALSE, echo=FALSE}

set.seed(42)
x <- model.matrix(class2 ~., npf_train)[,-1]
y <- npf_train$class2
x.test <- model.matrix(class2 ~., npf_valid)[,-1]

cv.lambda <- cv.glmnet(x, y, family = "binomial", alpha = 1)$lambda.min
m.lassoCV <- glmnet(x, y, family = "binomial", alpha = 1, lambda = cv.lambda)

#Train
phat_LGLCV_train <- predict(m.lassoCV, newx = x, type = "response")
#Test
phat_LGLCV <- predict(m.lassoCV, newx = x.test, type = "response")

#Cross-Validation
split <- c(rep_len(1:10, length.out = nrow(npf.all.clean)))
phat_LGLCV_CV <- c()
for (i in 1:10){
  x <- model.matrix(class2 ~., npf.all.clean[split!=i,])[,-1]
  y <- npf.all.clean$class2[split!=i]
  x.test <- model.matrix(class2 ~., npf.all.clean[split==i,])[,-1]
  mod <- glmnet(x, y, family = "binomial", alpha = 1, lambda = cv.lambda)
  phat_LGLCV_CV[split==i]<- predict(mod, newx = x.test, type = "response")
}

#Performance measures into the table
npf_results[nrow(npf_results)+1,] <- c("Log reg Lasso CV", 
                                       round(accuracy(phat_LGLCV_train, class2_num_train),3), 
                                       round(accuracy(phat_LGLCV, class2_num),3), 
                                       round(accuracy(phat_LGLCV_CV, class2_num_all),3), 
                                       round(perplexity(phat_LGLCV_train, npf_train$class2),3), 
                                       round(perplexity(phat_LGLCV, npf_valid$class2),3), 
                                       round(perplexity(phat_LGLCV_CV, npf.all.clean$class2),3))
cv.lambda
```

The estimated coefficients are

```{r, message=FALSE, echo=FALSE}
coef(m.lassoCV)
```
As we can see, some of the variables have a zero coefficient meaning that Lasso has done variable selection.

2) A "normal" decision tree selects the following variables with the misclassification as follows

```{r, message=FALSE, echo=FALSE}

set.seed(42)
tree.npf <- tree(class2~., npf_train)
summary(tree.npf)
```

```{r, message=FALSE, echo=FALSE}
plot(tree.npf)
text(tree.npf, pretty = 0, cex=0.6)
```
Accuracy of the tree can be calculated from the confusion matrix. Predicted classes vs. actual classes for validation data:

```{r, message=FALSE, echo=FALSE}
tree.pred <- predict(tree.npf, npf_valid, type = "class")
table(tree.pred, npf_valid$class2)
```
Predicted classes vs. actual classes for training data:
--- this can actually be seen from the results above (summary(tree.npf)) - do we want to leave this confusion matrix away?

```{r, message=FALSE, echo=FALSE}
tree.pred_train <- predict(tree.npf, npf_train, type = "class")
table(tree.pred_train, npf_train$class2)
```
The accuracies are respectively
```{r, message=FALSE, echo=FALSE}
acc_tree <- (113+107)/232
acc_tree_train <- (104+94)/232
npf_results[nrow(npf_results)+1,] <- c("Tree", round(acc_tree_train,3), round(acc_tree,3), "","","","")
round(acc_tree,3)
round(acc_tree_train,3)
```

3) Random Forest with 5 variables (square root of the number of features) used in each run

Confusion matrix for training data
```{r, message=FALSE, echo=FALSE}

set.seed(42)
rf.npf <- randomForest(class2~., data = npf_train, mtry = 5, importance = TRUE)
rf.pred_train <- predict(rf.npf, newdata = npf_train)
table(rf.pred_train, npf_train$class2)
```

Confusion matrix for validation
```{r, message=FALSE, echo=FALSE}
rf.pred <- predict(rf.npf, newdata = npf_valid)
table(rf.pred, npf_valid$class2)
```

```{r, message=FALSE, echo=FALSE}
acc_tree_train <- (115+117)/232
acc_tree <- (100+98)/232
npf_results[nrow(npf_results)+1,] <- c("Random Forest", round(acc_tree_train, 3), round(acc_tree,3), "", "", "","")
acc_tree_train
acc_tree
```
The importance of the variables are:

```{r, message=FALSE, echo=FALSE}
varImpPlot(rf.npf, cex=0.6, main = "Random Forest")
```

4) PCA
PCA is used to the original training data (npf_train.csv with 464 observations) together with the original testing data (npf_hidden.csv) where the variables are centered to have zero mean and scaled to have standard deviation one. The responses variables are removed from the original training data because we are using unsupervised learning method. The first two principal components are:

```{r, message=FALSE, eval=TRUE,echo=FALSE}
npf_pca <- rbind(npf[,-c(1:2)], npf_hidden[,-c(1:4)])
#the old data that was used in pca: npf_pca <- npf[,-c(1,102)]
pca.npf <- prcomp(npf_pca, scale = TRUE )
#if we get to know class4 in npf_hidden, we can use colouring according to class4
#autoplot(pca.npf,data=npf_pca,colour="class4",loadings=TRUE,loadings.label=TRUE)
autoplot(pca.npf,data=npf_pca,loadings=TRUE,loadings.label=TRUE)+theme_bw()+
  ggtitle("PCA of the variables")+
    theme(panel.grid.minor = element_blank(),plot.title = element_text(hjust = 0.5),
        legend.text=element_text(size=11),legend.text.align = 0, legend.position = c(0.5,0.78))
```

---Can we conclude from the picture above if there are any significant outliers?

The proportion of variance explained (PVE) by each principal component and the cumulative PVE is shown in the figures below:
```{r, message=FALSE, echo=FALSE,eval=FALSE}
par(mfrow=c(1,2))
pca.npf.var <- pca.npf$sdev^2
pve <- pca.npf.var/sum(pca.npf.var)
plot(pve, xlab = "Principal Component", ylab = "Proportion of variance Explained", ylim =c(0,1), type ="b")
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative Proportion of variance Explained", ylim =c(0,1), type ="b")

```

```{r, message=FALSE,echo=FALSE}
#The loadings of 5 first principal components are
#pca.npf$rotation[,1:5]
```

PCA is also done for the data where variables with high correleation to other variables are removed (cleaned data). The first two principal components are:

```{r, message=FALSE,echo=FALSE}
columns <- colnames(npf.clean)
npf_hidden_clean <- npf_hidden[,colnames(npf_hidden) %in% columns]
npf_pca_clean <- rbind(npf.all.clean[,-1], npf_hidden_clean)
pca.npf.clean <- prcomp(npf_pca_clean, scale = TRUE )
#if we get to know class4 in npf_hidden, we can use colouring according to class4 (or class2)
#autoplot(pca.npf,data=npf_pca_clean,colour="class4",loadings=TRUE,loadings.label=TRUE)
autoplot(pca.npf.clean,data=npf_pca_clean,loadings=TRUE,loadings.label=TRUE)+
  theme_bw()+
  ggtitle("PCA of the clean variables")+
    theme(panel.grid.minor = element_blank(),plot.title = element_text(hjust = 0.5),
        legend.text=element_text(size=11),legend.text.align = 0, legend.position = c(0.5,0.78))

#### Do we want to draw the pca using validation set to train or are we using it just to compare performance ?? 
#### I am just a bit confused between the sets :/
pca.npf.clean <- prcomp(npf.all.clean[,-1], scale = TRUE )
autoplot(pca.npf.clean,data=npf.all.clean,loadings=TRUE,loadings.label=TRUE,colour='class2')+theme_bw()

```

The proportion of variance explained (PVE) by each principal component and the cumulative PVE is shown in the figures below:

```{r, message=FALSE, echo=FALSE,eval=FALSE}
par(mfrow=c(1,2))
pca.npf.var <- pca.npf.clean$sdev^2
pve <- pca.npf.var/sum(pca.npf.var)
plot(pve, xlab = "Principal Component", ylab = "Proportion of variance Explained, cleaned data", ylim =c(0,1), type ="b")
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative Proportion of variance Explained, cleaned data", ylim =c(0,1), type ="b")

```

According to the results above at least 15 first principal components should be used to explaina bout 90 % of the variance.
--- If you can conclude something else from the results, you can add some text :)

The performance of the classifiers investigated so far are:

```{r, message=FALSE, echo=FALSE,eval=FALSE}
npf_results
```

## Conclusions and feature selection

--- !!! This whole part can be removed if it's not needed. The text is not updated after cleaning the data so at least the conclusions should be updated. The models in the next session is set to use cleaned data instead of the selection previously done here. !!!

The accuracy is best for Random Forest, but also Logistic Regression with Lasso where lambda is selected using CV ("log reg CV") performs very well.
---- Should we investigate diagnostic plots for log reg? Should be give weight to well performed classifiers when selecting the variables?

All models use the following variables 
- RHIRGA: mean is more important than std according to tree and RF
- H2O mean
- O3 mean (Logistic Regression with Lasso, lambda = CV ("log reg CV") gives small value to std)
- CO2: tree and RF uses only std, log regs give value to mean too
- T std
- SWS.mean 

The following variables are used in all other models but RF
- NO.std
- Pamb0.mean or .std

The following variables are used in all other models but "normal" tree
- CS.mean (.std only in log reg CV)

These variables are used in some models
- UV_B.std is used only on log reg CV 
- UV_A.mean is used only in RF
- RGlob.mean and RGlob.std (RF, mean also in log reg 0.1)
- PTG.std (tree, RF)
- NET.mean
- RPAR.mean
- SO2

Because the measures in different heights/levels are highly correlated, we select just one of them.
---- which ones? Should we put more weight to the selections in random forest and log reg CV?
---- Should be produce boxplots, histograms and/or scatterplots to selected variables

We select the following variables to be used in models:
---- selected for testing the same variables than in Log Reg Lasso CV but only once if there are multiple values with different heights ------------
```{r, message=FALSE, echo=FALSE}
#selection <- c("class2", "CO2168.std", "CO242.mean", "H2O42.mean", "NO168.std", "O3504.mean", "O3168.std", "Pamb0.std", "RHIRGA504.mean", "RHIRGA672.std", "SO2168.std", "SWS.mean", "T672.std", "UV_B.std", "CS.mean", "CS.std")
  #c("class2", "CO2168.mean","CO2168.std", "Glob.mean", "H2O168.mean", "H2O168.std", "NO168.std", "O3168.mean", "Pamb0.mean", "Pamb0.std", "PTG.std", "RGlob.mean", "RHIRGA168.mean", "RHIRGA168.std", "SWS.mean", "T168.std", "CS.mean", "CS.std")
#npf_train_trimmed <- npf_train[, colnames(npf_train) %in% selection]
#npf_valid_trimmed <- npf_valid[, colnames(npf_train) %in% selection]
#npf_trimmed <- npf[, colnames(npf) %in% selection]
#selection
```

Let's check if there are any correlation left:

```{r, message=FALSE, echo=FALSE}
#cm <- cor(npf_trimmed[,-16])
#colnames(cm) <- rownames(cm) <- sapply(colnames(cm), function(s) gsub(".mean","",s))
#corrplot(cm, order = "FPC", tl.cex=0.5, tl.col= "black")
```


## Model selection

```{r, message=FALSE, echo=FALSE}
#this is CV function from Problem 2 is below but I have not used it since both NB and logistic regression needs some modifications...
#Cross-Validation Predictions
#1) split n items into k fold of roughly equal size
kpart <- function(n,k){rep_len(1:k, lenght.out=n)}
#2) find cross-validation predictions
cv <- function(
    formula,
    data,
    model = lm,
    n = nrow(data),
    k = min(n, 10),
    split = kpart(n,k),
    train = function(data){model(formula, data = data)},
    pred = function(model, data){predict(model, newdata = data)}){
  phat <- NULL
  for (i in 1:k){
    mod <- train(data[split !=i, ])
    if (is.null(phat)){
      phat <- pred(mod, data)
    } else {
      phat[split==i]<- pred(mod, data[split==i,])
    }
  }
  phat
}

```

The models tested in addition to the ones already tested are

1) Dummy: 
---- write a description of the method and the code itself...

2) Naive Bayes
```{r, message=FALSE, echo=FALSE}

phat_NB <- function(npf_train,
npf_valid,
model = function(data) naiveBayes(class2 ~ ., data, laplace = 1),
pred = function(model, x) predict(model, x, type = "raw")[, 2]) {
  m <- model(npf_train)
  pred(m, npf_valid)
}

#Train
phat_NB_train <- phat_NB(npf_train, npf_train)
#Test
phat_NB <- phat_NB(npf_train, npf_valid)

#Cross-Validation
split <- c(rep_len(1:10, length.out = nrow(npf.all.clean)))
phat_NBCV <- c()
for (i in 1:10){
  mod <- naiveBayes(class2 ~ ., npf.all.clean[split !=i, ], laplace = 1)
  phat_NBCV[split==i]<- predict(mod, npf.all.clean[split==i,], type = "raw")[,2]
}

npf_results[nrow(npf_results)+1,] <- c("Naive Bayes", 
                                       round(accuracy(phat_NB_train, class2_num_train),3), 
                                       round(accuracy(phat_NB, class2_num),3), 
                                       round(accuracy(phat_NBCV, class2_num_all),3), 
                                       round(perplexity(phat_NB_train, npf_train$class2),3), 
                                       round(perplexity(phat_NB, npf_valid$class2),3), 
                                       round(perplexity(phat_NBCV, npf.all.clean$class2),3))

```

Accuracy of Naive Bayes when using the first 15 principal components to reduce the dimensionality of the data. The highest accuracy is marked with red.

```{r,message=FALSE,echo=FALSE}
#perfoming PCA to the whole, cleaned data (npf_train.csv and npf_hidden.csv cleaned from correlated variables) 
pca_class2 <- prcomp(npf_pca_clean, scale = TRUE)

#calculation of accuracy on data with reduced dimensionality, reduction done by using PCs from 1 to 15
acc_NB <- c()

for (i in 1:15){
  df_train <- data.frame(pca_class2$x[train_id,1:max(1,i),drop=FALSE], class2 = npf_train$class2)
  df_valid <- data.frame(pca_class2$x[-c(train_id, 465:1429),1:max(1,i),drop=FALSE])
  
  NB <- naiveBayes(class2 ~ ., df_train, laplace = 1)
  phat <- predict(NB, df_valid, type = "raw")[, 2]
  acc_NB <- append(acc_NB, accuracy(phat, class2_num))
}

plot(c(1,15),range(acc_NB),type="n",xlab="# of principal components",ylab="accuracy")
lines(1:15, acc_NB,type="b")
i <- which.max(acc_NB); points(i,acc_NB[i],pch=16,col="red")
legend("bottomright",c("accuracy"),lty=c("solid"),pch=c(1))
```

The highest accuracy is received when using 5 first PCs so this is added to be one of the possible methods.

```{r, message=FALSE, echo=FALSE}
#selected number of PC
PC <- c(5)
# accuracy and perplexity of the selected PC is added to the result table
df_train <- data.frame(pca_class2$x[train_id,1:PC,drop=FALSE], class2 = npf_train$class2)
df_valid <- data.frame(pca_class2$x[-c(train_id, 465:1429),1:PC,drop=FALSE])
  
NB <- naiveBayes(class2 ~ ., df_train, laplace = 1)

#Train
phat_NB_train <- predict(NB, df_train, type = "raw")[, 2]

#Validation
phat_NB <- predict(NB, df_valid, type = "raw")[, 2]

#Cross-Validation
split <- c(rep_len(1:10, length.out = nrow(npf.all.clean)))
phat_NBCV <- c()
for (i in 1:10){
  #first, npf_hidden is removed from the pc's x matrix
  df <- data.frame(pca_class2$x[-c(465:1429),1:PC, drop=FALSE])
  #training data to be used in CV - class2 is added to the data
  df_train <- data.frame(df[split != i,], class2 = npf.all.clean$class2[split != i])
  #validation data to be used in CV
  df_valid <- df[split ==i,]
  
  mod <- naiveBayes(class2 ~ ., df_train, laplace = 1)
  phat_NBCV[split==i]<- predict(mod, df_valid, type = "raw")[,2]
}

npf_results[nrow(npf_results)+1,] <- c("Naive Bayes with PCA", 
                                       round(accuracy(phat_NB_train, class2_num_train),3), 
                                       round(accuracy(phat_NB, class2_num),3), 
                                       round(accuracy(phat_NBCV, class2_num_all),3), 
                                       round(perplexity(phat_NB_train, npf_train$class2),3), 
                                       round(perplexity(phat_NB, npf_valid$class2),3),
                                       round(perplexity(phat_NBCV, npf.all.clean$class2),3))

```

3) Logistic regression
--- with or without interactions? now it's without
```{r, message=FALSE, echo=FALSE}

phat_LG <- function(npf_train,
npf_valid,
model = function(data) glm(class2 ~ ., data, family = "binomial"),
pred = function(model, x) predict(model, newdata = x, type = "response")) {
  m <- model(npf_train)
  pred(m, npf_valid)
}

#Train
phat_LG_train <- phat_LG(npf_train, npf_train)
#Test
phat_LG <- phat_LG(npf_train, npf_valid)

#Cross-Validation
split <- c(rep_len(1:10, length.out = nrow(npf.all.clean)))
phat_LGCV <- c()
for (i in 1:10){
  mod <- glm(class2 ~ ., npf.all.clean[split !=i, ], family = "binomial")
  phat_LGCV[split==i]<- predict(mod, newdata = npf.all.clean[split==i,], type = "response")
}

npf_results[nrow(npf_results)+1,] <- c("Logistic regression", 
                                       round(accuracy(phat_LG_train, class2_num_train),3), 
                                       round(accuracy(phat_LG, class2_num),3), 
                                       round(accuracy(phat_LGCV, class2_num_all),3), 
                                       round(perplexity(phat_LG_train, npf_train$class2),3), 
                                       round(perplexity(phat_LG, npf_valid$class2),3), 
                                       round(perplexity(phat_LGCV, npf.all.clean$class2),3))
```

4) k-NN
K nearest neighbor is tried with different values of k: 1, 5, 10, 15, 20 and 50. Accuracy on validation set for each of them are respectively

```{r, message=FALSE, echo=FALSE}

set.seed(42)

knn.acc <- c()

train.X <- scale(npf_train[,-1])
valid.X <- scale(npf_valid[,-1])
train.Y <- npf_train$class2
valid.Y <- npf_valid$class2
j=0

for (i in c(1, 5, 10, 15, 20, 50)){
  j=j+1
  knn.Y <- knn(train.X, valid.X, train.Y, k=i)
  knn.Y_num <- ifelse(knn.Y=="event",1,0)
  knn.acc[j] <- round(mean(ifelse(knn.Y_num==class2_num,1,0)),3)
}
knn.acc
```
When k = 5 (or 10), the accuracy is highest so 5-NN is added

```{r, message=FALSE, echo=FALSE}

set.seed(42)
k_sel <- c(5)
train.X <- scale(npf_train[,-1])
valid.X <- scale(npf_valid[,-1])
train.Y <- npf_train$class2
valid.Y <- npf_valid$class2

#Train
knn.Y_train <- knn(train.X, train.X, train.Y, k=k_sel)
knn.Y_train_num <- ifelse(knn.Y_train=="event",1,0)
knn.acc_train <- round(mean(ifelse(knn.Y_train_num==class2_num_train,1,0)),3)
#Validation
knn.Y <- knn(train.X, valid.X, train.Y, k=k_sel)
knn.Y_num <- ifelse(knn.Y=="event",1,0)
knn.acc_valid <- round(mean(ifelse(knn.Y_num==class2_num,1,0)),3)

#Cross-Validation predictions of the class
knn.Y_CV <- c()
for (i in 1:10){
  train.X <- scale(npf.all.clean[split!=i,-1])
  valid.X <- scale(npf.all.clean[split==i,-1])
  train.Y <- npf.all.clean$class2[split!=i]
  valid.Y <- npf.all.clean$class2[split==i]

  knn.Y_CV[split==i] <- knn(train.X, valid.X, train.Y, k=k_sel)
}

knn.Y_CV_num <- ifelse(knn.Y_CV=="event",1,0)
knn.acc_CV <- round(mean(ifelse(knn.Y_CV_num==class2_num_all,1,0)),3)

npf_results[nrow(npf_results)+1,] <- c("5-NN", 
                                       knn.acc_train, 
                                       knn.acc_valid,
                                       knn.acc_CV,
                                       "","","")
```



Performance measures for the methods are:
```{r, message=FALSE, echo=FALSE}
npf_results
```


```{r, message=FALSE, echo=FALSE,eval=FALSE}
#Just testing, if perplexity should be calculated in each iteration in CV and take an average of the 10 results. Test is done to logistic regression on selected data. CV Perplexity in the result table is 1.274 and now the result is

#split <- c(rep_len(1:10, length.out = nrow(npf_trimmed)))

#perplexity_testi <- c()

#for (i in 1:10){
 # phat_testi <- c()
  #mod <- glm(class2 ~ ., npf_trimmed[split !=i, ], family = "binomial")
  #phat_testi<- predict(mod, newdata = npf_trimmed[split==i,], type = "response")
  #perplexity_testi[i] <- perplexity(phat_testi, npf_trimmed$class2[split==i])
#}

#sum(perplexity_testi)/10

```

