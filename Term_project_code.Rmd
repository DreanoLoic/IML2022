---
title: "Term Project - Group 80"
author: "Sari Ropponen, Outi Boman, Loic Dreano"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE,warning=FALSE}
library(knitr)
library(ggplot2)
library(tibble)
library(class)
library(glmnet)
library(e1071)
library(ggfortify)
library(randomForest)
library(tree)
library(glmnet)
library(corrplot)
library(rstatix)
#library(reprtree)
opts_chunk$set(echo = TRUE)
#Value of repetion of CV
CV.rep=100
```

## Description of data

In the project a data about new particle formation, NPF, is used. The data is divided into training data (npf_train.csv) and testing data (npf_test_hidden.csv) and it includes 104 variables relating to daily measurements taken in Hyytiälä forestry field station. The number of observations in training data is 464 and 965 in testing data.

```{r, warning=FALSE,echo=FALSE,message=F,warning=FALSE}
npf <- read.csv("npf_train.csv")
npf_hidden <- read.csv("npf_test_hidden.csv")

df1=data_frame(" "=c("Measurements","Variables"))
df1$npf_train = dim(npf)
df1$npf_test = dim(npf_hidden)

kable(df1, digits = 3, caption = "Summary of the dataset")
# str(npf)
```

Some of the variables like temperature T and CO2 are measured in different heights. The height is indicated in the name of the variable, for example, T84.mean is the mean temperature at 8.4 meters above the mast base. The data includes also a response variable indicating if a NPF event has happened during the day or not. In the project a binary classifier is build to predict if an NPF event will happen or not during the day according to the observed measurements.

## Preprocessing data

The data includes also some variables that are not needed as variables. A summary of the the variables in training data is below:

```{r, message=FALSE,echo=FALSE}
summary(npf[,1:4])
# check that there is no variable with sd = 0 and remove them (only work for numerical or factor variable)
# npf.var = npf[, -which(apply(npf, 2, sd) == 0)]
# same goal but based on the length of the result of the unique function()
rownames(npf) <- npf[,"date"]
npf <- npf[, -c(1, 2)]
npf.var = npf[,-which(apply(npf, 2, function(x) length(unique(x))) == 1)] 
```

The column "date" was set to be the row names in the training data. Because the value of the logical variable "partlybad" is FALSE for all the observations, it doesn't give any information. Columns "id", "date" and "partlybad" were removed from the data. 

A qualitative variable "class2" is added to the training data. It gets either value "event" or "nonevent" according to "class4". Variable "class4" indicates the type of the event if it has happened ("Ia", "Ib" or "II") or "nonevent" if no event has happened during the day. 

```{r, message=FALSE,warning=FALSE, echo=FALSE}
npf.var$class2 <- factor("event", levels = c("nonevent", "event"))
npf.var$class2[npf$class4 =="nonevent"] <- "nonevent"
# reodering of npf.var to have the classes column at the beginning (easier to remove them after)
npf.var = npf.var[,c(1,102,2:101)]
```

## Data exploration

Because the data includes same measurements at different heights it is expected that there are correlation between variables. Correlations between different mean values are shown in the matrix below:

```{r, message=FALSE, warning=FALSE,echo=FALSE,fig.align="center"}

cm <- cor(npf.var[, endsWith(colnames(npf.var), ".mean")])
# Remove ".mean" from variable names
colnames(cm) <- rownames(cm) <- sapply(colnames(cm), function(s) gsub(".mean","",s))
corrplot(cm, order = "FPC", tl.cex=0.5, tl.col= "black",
          title="Correlation matrix of mean variables", 
     mar=c(0,0,1,0))

```
The same matrix for different standard deviation values is
```{r, message=FALSE, warning=FALSE,echo=FALSE,fig.align="center"}
# the correlation for the standard deviation variable 
cm_sd <- cor(npf.var[, endsWith(colnames(npf.var), ".std")])
colnames(cm_sd) <- rownames(cm_sd) <- sapply(colnames(cm_sd), function(s) gsub(".std","",s))
corrplot(cm_sd, order = "FPC", tl.cex=0.5, tl.col= "black",
                   title="Correlation matrix of std variables", 
     mar=c(0,0,1,0))
```

As we can see, there are a lot of highly correlated variables and since they carry the same information, keeping all of them would not improve the predictivity of our models. In order to increase the power of our models to identify independent variables that are statistically significant and to make them simpler to interpret (simpler model in general) we will remove the highly correlated variables.

To do so the variables are clustered together based on their correlation; every pairs of variables which have an absolute correlation greater than 0.8 are clustered together. Then a random variable from each cluster is kept for further analysis.

The correlations for mean and standard deviations after cleaning the data are shown in the pictures below as well as a table of variables left in the data (table 2). Table 3 shows that the data includes only 26 variables. Histograms of the variables left in the cleaned data are given in the annex. The histograms give us an idea of the values of the variables which we need, for example, in PCA to decide whether to normalize and scale the data or not.

```{r, clean based on correlation,message=FALSE,warning=FALSE, echo=FALSE}
# Function that remove variables if the correlation is greater than s (have to adapt)
elimcor_sansY<-function(X,s=0.80){
  #X Matrix of variable to cluster 
  #s limit value of correlation 
  correl=cor(X)
  stop=F
  possetap=1:ncol(X)
  groupes=as.list(1:ncol(X))
  
  while (stop==F)
  {
    ##Grouping of var based on |corr|>0.95
    gplist<-list(NULL)
    possglob=1:ncol(correl)
    for (i in 1:(ncol(correl)))
    {
      poss=possglob[-i]
      gplist[[i]]=c(i,poss[abs(correl[i,poss])>s])
    }
    ## sort the groups from bigger to smaller
    gplisteff=unlist(lapply(gplist,length))
    if (any(gplisteff>1))
    {
      gplistfin=gplist[gplisteff>1]
      gplistuniq=unlist(gplist[gplisteff==1])
      gpsel=NULL
      ## pick randomly one variable from each group to keep
      for (i in 1:length(gplistfin))
      {
        selloc=min(gplistfin[[i]])
        gploc=groupes[[possetap[selloc]]]
        for (j in 1:length(gplistfin[[i]]))
        {
          gploc=c(gploc,groupes[[possetap[gplistfin[[i]][j]]]])				    }
        groupes[[possetap[selloc]]]=unique(gploc)
        gpsel=c(gpsel,selloc)
      }
      possetap=possetap[c(gplistuniq,unique(gpsel))]
      correl=cor(X[,possetap])
    }
    else stop=T	
  }
  return(list(possetap=possetap,groupes=groupes))
}

npf.corr= npf.var[,-c(1,2)]

# ind.corr.90 = elimcor_sansY(npf.corr,0.90)$possetap
ind.corr.80 = elimcor_sansY(npf.corr,0.80)$possetap
var80 = colnames(npf.corr)[ind.corr.80]
# npf.clean = npf.corr[ind.corr.90] # 34 variables

npf.clean = npf.corr[var80] # 26 variables 
npf.test.clean = npf_hidden[var80]
```

```{r,message=FALSE,warning=FALSE, echo=FALSE}
#cm.2 <- cor(npf.clean[, endsWith(colnames(npf.clean), ".mean")])
#colnames(cm.2) <- rownames(cm.2) <- sapply(colnames(cm.2), function(s) gsub(".mean","",s))
#corrplot(cm.2, method = 'circle', type = 'upper', insig='blank',
#         addCoef.col ='black', order = 'FPC', diag=FALSE, 
#         tl.cex = 0.5, tl.col = "black",number.cex = 0.5,
#         title="Correlation matrix of mean variables for the clean dataset", 
#         mar=c(0,0,1,0))

# corrplot(cm.2, order = "FPC", tl.cex=0.5, tl.col= "black")
#cm.2_sd <- cor(npf.clean[, endsWith(colnames(npf.clean), ".std")])
#colnames(cm.2_sd) <- rownames(cm.2_sd) <- sapply(colnames(cm.2_sd), function(s) gsub(".std","",s))
#corrplot(cm.2_sd, method = 'circle', type = 'upper', insig='blank',
#         addCoef.col ='black', order = 'FPC', diag=FALSE, 
#         tl.cex = 0.5, tl.col = "black",number.cex = 0.5,
#         title="Correlation matrix of std variables for the clean dataset", 
#         mar=c(0,0,1,0))

#Option 2
#corrplot.mixed(cm.2, order = 'FPC',
#         tl.cex = 0.5, tl.col = "black",number.cex = 0.5,
#         title="Correlation matrix of mean variables for the clean dataset",
#         mar=c(0,0,1,0))
#corrplot.mixed(cm.2_sd, order = 'FPC',
#         tl.cex = 0.5, tl.col = "black",number.cex = 0.5,
#         title="Correlation matrix of std variables for the clean dataset",
#         mar=c(0,0,1,0))
```

```{r,message=FALSE,warning=FALSE, echo=FALSE}
df2=data_frame(" "=c("Measurements","Variables"))
df2$npf_train = dim(npf.clean)
df2$npf_test = dim(npf.test.clean)

#Option 3  
cm_3 <- cor(npf.clean)
corrplot(cm_3, method = 'circle', type = 'upper', insig='blank',
         order = 'FPC', diag=FALSE, 
         tl.cex = 0.5, tl.col = "black",number.cex = 0.35,
         # addCoef.col = "black",
         title="Correlation matrix of variables for the clean dataset", 
         mar=c(0,0,1,0) )

df3 = data.frame(var80[1:7],var80[8:14],var80[15:21],c(var80[21:26],""))
colnames(df3)= rep("",4)

kable(df3,caption = "List of kept variables")

kable(df2, digits = 3,caption = "Summary of clean dataset")

```


## Performance measures

To compare different classifiers two measures are used: accuracy and perplexity. Accuracy is the proportion of the observations that has been classified correctly. Perplexity is a rescaled variant of log-likelihood. If perplexity equals to 1 the classifier predicts always the probability of an observation to an actual class. Perplexity of 2 corresponds to coin flipping.

If the method predicts a probability of an event, observation is classified as "event" if the estimated posterior probability is more than 0.5. Otherwise the observation is classified as "nonevent".

Performance measures are calculated using validation method and 10-fold Cross-Validation. For validation method the training data is randomly divided into 2 equally sized data sets: training data to fit the model and validation data to estimate the accuracy and perplexity. Accuracy and perplexity are also calculated in training data for comparison and to evaluate if there is overfitting. 

Generalization accuracy and perplexity is calculated using cross-validation. The data is randomly divided into 10 folds and performance measures are calculated using each fold as a validation data in turn. The performance measures are the means of the 10 validation results.

```{r, message=FALSE,warning=FALSE, echo=FALSE}

# which training set ? 
npf.all.clean = cbind(class2=npf.var$class2,npf.clean) # data without correlated for class2
# npf = cbind(class2=npf.var$class4,npf.clean) # data without correlated for class4
# npf.all.full = npf.var[,-1] # Raw data for class2
# npf = npf.var[,-2] # Raw data for class4

set.seed(42)
train_id <- sample(1:nrow(npf), 232)
npf_train <- npf.all.clean[train_id,]
npf_valid <- npf.all.clean[-train_id,]


# to calculate mean, class2 needs to be converted into numeric values
class2_num_train <- c(ifelse(npf_train$class2=="event",1,0))
class2_num <- c(ifelse(npf_valid$class2=="event",1,0))
class2_num_all <- c(ifelse(npf.all.clean$class2=="event",1,0))
options(digits=10)
## accuracy if we know the probabilities of ones
accuracy <- function(p, y) mean(ifelse(p >= 0.5, 1, 0) == y)
## perplexity if we know the probabilities of ones
perplexity <- function(p, y) {
  ind.1= which(p=="1")
  p[ind.1]=0.999999999
  ind.0= which(p=="0")
  p[ind.0]=0.000000001
  exp(-mean(log(ifelse(y == "event", p, 1 - p))))
}

#initialization of result table
npf_results <- data.frame(matrix(ncol = 7))
colnames(npf_results) = c("Model", "Train Accuracy", "Validation Accuracy","CV Accuracy", "Train Perplexity", "Validation Perplexity", "CV Perplexity")
npf_results <- na.omit(npf_results)
```

## Investigation of the variables: PCA

Principal Component Analysis (PCA) is used to study how much we can reduce dimensionality of the data but to save as much of the variability (that is, information) of the data at the same time. We do the PCA using the original training data (npf_train.csv) combined with the original testing data (npf_hidden.csv). As we can see from the histograms of the variables (see the annex), the variables are measured in different units with different magnitudes of variances. Thus, the variables are centered to have zero mean and scaled to have standard deviation one. The responses variable are removed from the data because we are using unsupervised learning method. 

First, we do PCA to the original data before cleaning the correlated variables. The biplot of the analysis is below:

```{r, message=FALSE, eval=TRUE,echo=FALSE,warning=FALSE}
## Option 1 with color -> info training and validation sets seems to cover the same space
npf_pca <- rbind(npf.var[,-c(1)],  cbind("class2"=NA,npf_hidden[,-c(1:4)]))

#the old data that was used in pca: npf_pca <- npf[,-c(1,102)]
pca.npf <- prcomp(npf_pca[,-1], scale = TRUE )
#if we get to know class4 in npf_hidden, we can use colouring according to class4
#autoplot(pca.npf,data=npf_pca,colour="class4",loadings=TRUE,loadings.label=TRUE)
autoplot(pca.npf,data=npf_pca,colour="class2",loadings=TRUE,loadings.label=TRUE)+theme_bw()+
  ggtitle("PCA of the variables")+
    theme(panel.grid.minor = element_blank(),plot.title = element_text(hjust = 0.5),
        legend.text=element_text(size=11),legend.text.align = 0, legend.position = "bottom")
```

```{r, message=FALSE, eval=TRUE,echo=FALSE,warning=FALSE}
## Option 2 
#npf_pca <- rbind(npf[,-c(1:2)], npf_hidden[,-c(1:4)])
#the old data that was used in pca: npf_pca <- npf[,-c(1,102)]
#pca.npf <- prcomp(npf_pca, scale = TRUE )
#if we get to know class4 in npf_hidden, we can use colouring according to class4
#autoplot(pca.npf,data=npf_pca,colour="class4",loadings=TRUE,loadings.label=TRUE)
#autoplot(pca.npf,data=npf_pca,loadings=TRUE,loadings.label=TRUE)+theme_bw()+
#  ggtitle("PCA of the variables")+
#    theme(panel.grid.minor = element_blank(),plot.title = element_text(hjust = 0.5),
#        legend.text=element_text(size=11),legend.text.align = 0, legend.position = c(0.5,0.78))
```

The biplot includes the first two principal components (PCs), both the scores and loading vectors. Also, the proportion of variance explained (PVE) by the PC is indicated. As we can see, many of the vectors are overlapping meaning that they are correlated. This gives us a further justification to remove the correlated variables and leaving only one of each in the data. We can also see that the first PC (PC1) explains only 39.61 % of the variance in the data and the second PC (PC2) 14,95 %. Together they explain only 54,56 % of the variance which is quite poor result the target being more than 80 %.

From the biplot we can also see that there are a couple of outliers in the bottom right corners. For now, we leave them to be and handle them during modelling if needed.

To see how many PCs are needed to explain more than 80 % of the variance the PVE by each principal component and the cumulative PVE is shown in the figures below:


```{r, message=FALSE, warning=FALSE,echo=FALSE,eval=TRUE}
par(mfrow=c(1,2))
pca.npf.var <- pca.npf$sdev^2
pve <- pca.npf.var/sum(pca.npf.var)
plot(pve, xlab = "Principal Component", ylab = "PVE", ylim =c(0,1), type ="b")
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative PVE", ylim =c(0,1), type ="b")
```

For the original data we would need 10-20 PCs to explain most of the variance.

To see how the results are changed after cleaning the data from correlated variables PCA is also done for the cleaned data. The biplot is shown below:

```{r, message=FALSE,echo=FALSE}

## Option 1 with color -> info training and validation sets seems to cover the same space
columns <- colnames(npf.clean)
npf_hidden_clean <- cbind("class2"=NA, npf_hidden[,colnames(npf_hidden) %in% columns])
npf_pca_clean <- rbind(npf.all.clean,npf_hidden_clean)
pca.npf.clean <- prcomp(npf_pca_clean[,-1], scale = TRUE )
#if we get to know class4 in npf_hidden, we can use colouring according to class4 (or class2)
#autoplot(pca.npf,data=npf_pca_clean,colour="class4",loadings=TRUE,loadings.label=TRUE)
autoplot(pca.npf.clean,data=npf_pca_clean,loadings=TRUE,colour="class2",loadings.label=TRUE)+
  theme_bw()+
  ggtitle("PCA of the clean variables")+
    theme(panel.grid.minor = element_blank(),plot.title = element_text(hjust = 0.5),
        legend.text=element_text(size=11),legend.text.align = 0, legend.position = "bottom")
```

```{r, message=FALSE,echo=FALSE}
## Option 2 
#columns <- colnames(npf.clean)
#npf_hidden_clean <- npf_hidden[,colnames(npf_hidden) %in% columns]
#npf_pca_clean <- rbind(npf.all.clean[,-1], npf_hidden_clean)
#pca.npf.clean <- prcomp(npf_pca_clean, scale = TRUE )
#if we get to know class4 in npf_hidden, we can use colouring according to class4 (or class2)
#autoplot(pca.npf,data=npf_pca_clean,colour="class4",loadings=TRUE,loadings.label=TRUE)
#autoplot(pca.npf.clean,data=npf_pca_clean,loadings=TRUE,loadings.label=TRUE)+
#  theme_bw()+
#  ggtitle("PCA of the clean variables")+
#    theme(panel.grid.minor = element_blank(),plot.title = element_text(hjust = 0.5),
#        legend.text=element_text(size=11),legend.text.align = 0, legend.position = c(0.5,0.78))

```

The loading vectors are not as overlapped as previously but the PVE is still quite poor: the two first PCs explains only 38,61 % of the variance. 

Again, PVE by each principal component and the cumulative PVE are investigated:

```{r, message=FALSE, warning=FALSE,echo=FALSE,eval=TRUE}
par(mfrow=c(1,2))
pca.npf.var <- pca.npf.clean$sdev^2
pve <- pca.npf.var/sum(pca.npf.var)
plot(pve, xlab = "Principal Component", ylab = "PVE, cleaned data", ylim =c(0,1), type ="b")
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative PVE, cleaned data", ylim =c(0,1), type ="b")

```

We can see that about 10 first principal components should be used to explain about 80 % of the variance.

The PCA shows that we could reduce the dimensionality of the data from 26 variables (in cleaned data) to some extend. We are going to do that later by using the results of the PCA together with Naive Bayes. In next section we first investigate some other methods which include also feature selections.

## Models including feature selection

Some methods include variable selection in itself, like Lasso and decision trees. We investigate logistic regression with Lasso and random forest. The accuracy of the approaches are also calculated as well as perplexity of logistic regression with Lasso.

1) Logistic regression with Lasso

Logistic regression is a discriminant classifier which assumes that the log odds is linear in variables. When combined with Lasso a subset selection of variables are done by adding so called penalty term to residual sum of squares which is minimized in parameter estimation process. The bigger the estimated coefficients of the variables are the bigger the penalty. The penalty term forces some of the coefficients to zero. As we can see from the histograms in the annex the variables are measured in different units and scales. To make sure that the magnitude of a variable does not give too much weight we use data that is normalized to have zero mean and scaled to have unit variance.

The amount of penalty depends on a coefficient called lambda. The value of lambda is selected by cross-validation so that the value of the test error is minimized. For example, in valdiation approach the selected lambda is

```{r,message=FALSE,warning=FALSE,echo=FALSE}
# Groups creator
split.CV = function(Mat,nbG){
  ###Mat : matrix to split
  ##nbG : Number of group to build 
  # Sample make it random
  Ech.sample=sample(dim(Mat)[1])
  vect=ceiling(seq(1,dim(Mat)[1],length=(nbG+1)))
  
  # empty list
  echlist=as.list(rep(0,nbG))
  
  for(i in 1:length(echlist)){
    if (i!=nbG){ echlist[[i]]=Ech.sample[vect[i]:((vect[i+1])-1)] }
    if (i==nbG){ echlist[[i]]=Ech.sample[vect[i]:length(Ech.sample)] }
  }
  return (echlist)
}
```


```{r, message=FALSE,warning=FALSE, echo=FALSE}

set.seed(42)
#x <- model.matrix(class2 ~., npf_train)[,-1]
#x.test <- model.matrix(class2 ~., npf_valid)[,-1]

x <- scale(as.matrix(npf_train[,-1]))
y <- npf_train$class2
x.test <- scale(as.matrix(npf_valid[,-1]))

cv.lambda <- cv.glmnet(x, y, family = "binomial", alpha = 1)$lambda.min
m.lassoCV <- glmnet(x, y, family = "binomial", alpha = 1, lambda = cv.lambda)

#Train
phat_LGLCV_train <- predict(m.lassoCV, newx = x, type = "response")
#Test
phat_LGLCV <- predict(m.lassoCV, newx = x.test, type = "response")

#Cross-Validation
# split <- c(rep_len(1:10, length.out = nrow(npf.all.clean)))
l.acc.LGLCV = NULL
l.perp.LGLCV = NULL
for (l in seq(CV.rep)){
  split = split.CV(npf.all.clean,10)
  phat_LGLCV_CV <- c()
  for (i in 1:10){
    ind.test = split[[i]]
    ind.all = unlist(split)
    ind.train = ind.all[!ind.all %in% ind.test]
    #x <- model.matrix(class2 ~., npf.all.clean[ind.train,])[,-1]
    x <- scale(as.matrix(npf.all.clean[ind.train,-1]))
    y <- npf.all.clean$class2[ind.train]
    #x.test <- model.matrix(class2 ~., npf.all.clean[ind.test,])[,-1]
    x.test <- scale(as.matrix(npf.all.clean[ind.test,-1]))
      
    cv.lambda.i <- cv.glmnet(x, y, family = "binomial", alpha = 1)$lambda.min
    mod <- glmnet(x, y, family = "binomial", alpha = 1, lambda = cv.lambda.i)
    phat_LGLCV_CV[ind.test]<- predict(mod, newx = x.test, type = "response")
  }
  l.acc.LGLCV=c(l.acc.LGLCV,accuracy(phat_LGLCV_CV, class2_num_all))
  l.perp.LGLCV=c(l.perp.LGLCV,perplexity(phat_LGLCV_CV, npf.all.clean$class2))
}

#Performance measures into the table
npf_results[nrow(npf_results)+1,] <- c("Log reg Lasso", 
                                       round(accuracy(phat_LGLCV_train, class2_num_train),3), 
                                       round(accuracy(phat_LGLCV, class2_num),3), 
                                       round(mean(l.acc.LGLCV),3), 
                                       round(perplexity(phat_LGLCV_train, npf_train$class2),3), 
                                       round(perplexity(phat_LGLCV, npf_valid$class2),3), 
                                       round(mean(l.perp.LGLCV),3))
round(cv.lambda,3)
```

The estimated coefficients with the selected lambda in training data (with 232 observations) are

```{r, message=FALSE,warning=FALSE, echo=FALSE}
m.lassoCV %>% tidy() %>% kable(digits = 3)
#coef(m.lassoCV)
```
As we can see, a variable selection has been done since only 15 variables have a nonzero coefficient and some of them are still close to zero. 

2) Random Forest

Decision trees are learning methods that segment observations into regions. The segmentation is done recursively using binary decision rules which minimize the selected measure like residual sum of squares (RSS) in case of regression tree or e.g. classification error rate or Gini index in case of classification tree. A prediction for an observation belonging to a certain region is given by the mean of responses of the training observations in the same region (regression tree) or by the label to which the majority of the training observations belong to in the same region (classification tree). 

Random Forest builds a number of decision trees using each time a sample of training data, sampling done by bootstrapping. Predictions are averages of the resulting trees. During each split only one of the given number of randomly selected variables are considered. By reducing the number of variables the correlation between the trees is reduced.

To get predictions of probabilities of event given the observations when using random forest we change the response variable (class2) to a dummy variable: it gets a value 1 when an event has occurred and value 0 in case of nonevent. Nine variables are considered during each split. The selected number equals to the commonly used one third of the variables in the data in case of regression tree.

Confusion matrix for training data is
```{r, message=FALSE,warning=FALSE, echo=FALSE}
set.seed(42)
npf_train.rf = npf_train
npf_train.rf$class2=class2_num_train
#in case of classification tree the number of variables should be 26^0.5 and in regression 26/3. Assuming we have regression because we are using dummy response, we choose 9 
rf.npf <- randomForest(class2~., data = npf_train.rf, mtry = 9, importance = TRUE)
phat_RF_train <- predict(rf.npf, newdata = npf_train.rf)

kable(table(ifelse(phat_RF_train >= 0.5, 1, 0), npf_train$class2))
```

As we can see, no misclassification is done in training data. To see how well the model performs on validation set, we look at the confusion matrix for validation data set:

```{r test for tree plot, message=FALSE,warning=FALSE, echo=FALSE}

 plot.getTree <- function(rforest=NULL,tr=NULL,k=1, depth=0,main=NULL,caret=F, ...){
  require(randomForest)
  if(caret){
    mod<-rforest
    rforest<-mod$finalModel
  }
  
  if(is.null(rforest) && is.null(tr))stop('One of a random forest object or a tree object must be input')
  if(!is.null(rforest)){
    gTree <- getTree(rforest, k=k, labelVar=TRUE)
    
    if(caret){
      x <- as.tree(gTree, mod, caret=T)
    }
    else{
      x <- as.tree(gTree, rforest)
    }
    
  } else {
    x <- tr
  }
  if(depth>0){
    x <- snip.depth(x,depth)
  }
  x
  plot(x, type='uniform')
  text(x)
  # labelBG(x)
  # labelYN(x)
  title(main=main)
 }
 #plot.getTree(rf.npf,k=9)

 
 

```

```{r, message=FALSE,warning=FALSE, echo=FALSE}
phat_RF <- predict(rf.npf, newdata = npf_valid)

kable(table(ifelse(phat_RF >= 0.5, 1, 0), npf_valid$class2))
```

There are 19 observations predicted to be nonevent even though the event has occurred and respectively 16 observations given the false prediction of event. All together, the accuracy in validation set is 84,9 %.

```{r, message=FALSE,warning=FALSE, echo=FALSE}
#Cross-Validation
# split <- c(rep_len(1:10, length.out = nrow(npf.all.clean)))
l.acc.RF_CV = NULL
l.perp.RF_CV = NULL
for (l in seq(CV.rep)){
  split = split.CV(npf_train.rf,10)
  phat_RF_CV <- c()
  for (i in 1:10){
    ind.test = split[[i]]
    ind.all = unlist(split)
    ind.train = ind.all[!ind.all %in% ind.test]
    # x <- model.matrix(class2 ~., npf.all.clean[ind.train,])[,-1]
    # y <- npf.all.clean$class2[ind.train]
    rf.npf_CV <- randomForest(class2~., data = npf_train.rf[ind.train,], mtry = 9, importance = TRUE)
    phat_RF_CV<- predict(rf.npf_CV, newdata = npf_train.rf[ind.test,])
    # x.test <- model.matrix(class2 ~., npf.all.clean[ind.test,])[,-1]
    # mod <- glmnet(x, y, family = "binomial", alpha = 1, lambda = cv.lambda)
    # phat_RF_CV[ind.test]<- predict(mod, newx = x.test, type = "response")
  }
  l.acc.RF_CV = c(l.acc.RF_CV,accuracy(phat_RF_CV, class2_num_all))
  l.perp.RF_CV = c(l.perp.RF_CV,perplexity(phat_RF_CV, npf.all.clean$class2))
}
l.perp.RF_CV=l.perp.RF_CV[-which(is.nan(l.perp.RF_CV))]
#Performance measures into the table
npf_results[nrow(npf_results)+1,] <- c("Random Forest", 
                                       round(accuracy(phat_RF_train, class2_num_train),3), 
                                       round(accuracy(phat_RF, class2_num),3), 
                                       round(mean(l.acc.RF_CV),3), 
                                       round(perplexity(phat_RF_train, npf_train$class2),3), 
                                       round(perplexity(phat_RF, npf_valid$class2),3), 
                                       round(mean(l.perp.RF_CV),3))
```

The importance of variables are:

```{r, message=FALSE,warning=FALSE, echo=FALSE}
varImpPlot(rf.npf, cex=0.6, main = "Random Forest")
```

We can see that random forest do some kind of a variable selection since only some of the variables have a significant importance in forming the regions and thus, the predictions. However, the most important variables are not the same as with logistic regression with Lasso.
--- we should add explanations to %incMSE and IncNodePurity and do some conclusions of the pictures

The performance of the classifiers investigated so far are:

```{r, message=FALSE, warning=FALSE,echo=FALSE,eval=TRUE}
kable(npf_results)
```

Even though the accuracy of random forest in validation set is better than logistic regression with Lasso the performance estimated by cross-validation is poor. Especially, cross-validated perplexity of logistic regression with Lasso seems to be very good.

## Other models

The models tested in addition to the logistic regression with Lasso and random forest are a dummy model, Naive Bayes with reduced dimensionality, logistic regression using all variables in cleaned data (to compare the results with the one with Lasso) and k nearest neighbor.

1) Dummy

```{r, message=FALSE,warning=FALSE, echo=FALSE}

# Finding out the main class in the training data
train_main_value <- ifelse(length(npf_train$class2[npf_train$class2=="event"])/length(npf_train$class2)>=0.5, 'event','nonevent')
 
# The ratio of the value class to the total in the data
Dummy_accuracy <- function(data, value){
length(data[data==value])/length(data)
}
#Training data
phat_Dummy_train <- Dummy_accuracy(npf_train$class2, train_main_value)
# Validation data
phat_Dummy <- Dummy_accuracy(npf_valid$class2, train_main_value)

#Cross-Validation

l.acc.DummyCV = NULL
l.perp.DummyCV = NULL
for (l in seq(CV.rep)){
  split = split.CV(npf.all.clean,10)
# Here is a data frame for each split to have the probability and the main event class
# And it works! I didn't know how to use your other loop though for mean CV perplexity!
  df_DummyCV <- data.frame()
  for (i in 1:10){   
    ind.test = split[[i]]
    ind.all = unlist(split)
    ind.train = ind.all[!ind.all %in% ind.test]
    y <- npf.all.clean$class2[ind.train]
    train_main_value <- ifelse((length(y[y=="event"])/length(y))>=0.5, 'event','nonevent')
    new <- c(as.numeric(Dummy_accuracy(y, train_main_value)), train_main_value)
    df_DummyCV <- rbind(df_DummyCV, new)
     # Dummy_accuracy(new, train_main_value)
    l.perp.DummyCV = c(l.perp.DummyCV,perplexity(Dummy_accuracy(y, train_main_value), train_main_value))
  }
  colnames(df_DummyCV) <-c("phat", "tmv")
  df_DummyCV$phat=as.numeric(df_DummyCV$phat)
  phat_DummyCV <- mean(df_DummyCV$phat)
  perp_DummyCV <- perplexity(df_DummyCV$phat, df_DummyCV$tmv)
  l.acc.DummyCV = c(l.acc.DummyCV,phat_DummyCV)
  l.perp.DummyCV = c(l.perp.DummyCV,perp_DummyCV)
}




npf_results[nrow(npf_results)+1,] <- c("Dummy", 
                                       round(phat_Dummy_train,3), 
                                       round(phat_Dummy,3), 
                                       round(phat_DummyCV,3), 
                                       round(perplexity(phat_Dummy_train, train_main_value),3), 
                                       round(perplexity(phat_Dummy, train_main_value),3),
                                       round(mean(l.perp.DummyCV),3))



```


2) Naive Bayes

Naive Bayes is a generative classifier which assumes that the variables in each class (event or nonevent) are independent. It can be assumed that this is not the case in NPF data, but Naive Bayes can still produce quite decent results. Naive Bayes assumes that the variables are from Gaussian distribution. Thus, no scaling of the data is needed since it doesn't affect the predictions.

```{r, message=FALSE,warning=FALSE, echo=FALSE}

phat_NB <- function(npf_train,
npf_valid,
model = function(data) naiveBayes(class2 ~ ., data, laplace = 1),
pred = function(model, x) predict(model, x, type = "raw")[, 2]) {
  m <- model(npf_train)
  pred(m, npf_valid)
}

#Train
phat_NB_train <- phat_NB(npf_train, npf_train)
#Test
phat_NB <- phat_NB(npf_train, npf_valid)

#Cross-Validation
# split <- c(rep_len(1:10, length.out = nrow(npf.all.clean)))
l.acc.NBCV = NULL
l.perp.NBCV = NULL
for (l in seq(CV.rep)){
  split = split.CV(npf.all.clean,10)
  phat_NBCV <- c()
  for (i in 1:10){   
    ind.test = split[[i]]
    ind.all = unlist(split) 
    ind.train = ind.all[!ind.all %in% ind.test]
    mod <- naiveBayes(class2 ~ ., npf.all.clean[ind.train, ], laplace = 1)
    phat_NBCV[ind.test]<- predict(mod, npf.all.clean[ind.test,], type = "raw")[,2]
  }
  l.acc.NBCV = c(l.acc.NBCV,accuracy(phat_NBCV, class2_num_all))
  l.perp.NBCV = c(l.perp.NBCV,perplexity(phat_NBCV, npf.all.clean$class2))
}

npf_results[nrow(npf_results)+1,] <- c("Naive Bayes", 
                                       round(accuracy(phat_NB_train, class2_num_train),3), 
                                       round(accuracy(phat_NB, class2_num),3), 
                                       round(mean(l.acc.NBCV),3), 
                                       round(perplexity(phat_NB_train, npf_train$class2),3), 
                                       round(perplexity(phat_NB, npf_valid$class2),3), 
                                       round(mean(l.perp.NBCV),3))

```

In addition to using Naive Bayes with all 26 variables in the cleaned data, we use Naive Bayes after reducing dimensionality by PCA. This means, that we use the selected number of the principal component score vectors as an input to the model. To decide the number of the PCs to use, we look at the accuracy of the model in validation data with different number of first PCs used (see the picture below). The highest accuracy is marked with red.

```{r,message=FALSE,warning=FALSE,echo=FALSE}
#perfoming PCA to the whole, cleaned data (npf_train.csv and npf_hidden.csv cleaned from correlated variables) 
pca_class2 <- prcomp(npf_pca_clean[,-1], scale = TRUE)

#calculation of accuracy on data with reduced dimensionality, reduction done by using PCs from 1 to 15
acc_NB <- c()

for (i in 1:15){
  df_train <- data.frame(pca_class2$x[train_id,1:max(1,i),drop=FALSE], class2 = npf_train$class2)
  df_valid <- data.frame(pca_class2$x[-c(train_id, 465:1429),1:max(1,i),drop=FALSE])
  
  NB <- naiveBayes(class2 ~ ., df_train, laplace = 1)
  phat <- predict(NB, df_valid, type = "raw")[, 2]
  acc_NB <- append(acc_NB, accuracy(phat, class2_num))
}

plot(c(1,15),range(acc_NB),type="n",xlab="# of principal components",ylab="accuracy")
lines(1:15, acc_NB,type="b")
i <- which.max(acc_NB); points(i,acc_NB[i],pch=16,col="red")
legend("bottomright",c("accuracy"),lty=c("solid"),pch=c(1))
```

The highest accuracy is received when using the five first PCs so this model is added to the comparison of methods.

```{r, message=FALSE, warning=FALSE,echo=FALSE}
#selected number of PC
PC <- c(5)
# accuracy and perplexity of the selected PC is added to the result table
df_train <- data.frame(pca_class2$x[train_id,1:PC,drop=FALSE], class2 = npf_train$class2)
df_valid <- data.frame(pca_class2$x[-c(train_id, 465:1429),1:PC,drop=FALSE])
  
NB <- naiveBayes(class2 ~ ., df_train, laplace = 1)

#Train
phat_NB_train <- predict(NB, df_train, type = "raw")[, 2]

#Validation
phat_NB <- predict(NB, df_valid, type = "raw")[, 2]

#Cross-Validation
l.acc.NBCV = NULL
l.perp.NBCV = NULL
for (l in seq(CV.rep)){
  split = split.CV(npf.all.clean,10)
  phat_NBCV <- c()
  for (i in 1:10){   
    ind.test = split[[i]]
    ind.all = unlist(split)
    ind.train = ind.all[!ind.all %in% ind.test]
    #first, npf_hidden is removed from the pc's x matrix
    df <- data.frame(pca_class2$x[-c(465:1429),1:PC, drop=FALSE])
    #training data to be used in CV - class2 is added to the data
    df_train <- data.frame(df[ind.train,], class2 = npf.all.clean$class2[ind.train])
    #validation data to be used in CV
    df_valid <- df[ind.test,]
    
    mod <- naiveBayes(class2 ~ ., df_train, laplace = 1)
    phat_NBCV[ind.test]<- predict(mod, df_valid, type = "raw")[,2]
  }
  l.acc.NBCV = c(l.acc.NBCV,accuracy(phat_NBCV, class2_num_all))
  l.perp.NBCV = c(l.perp.NBCV,perplexity(phat_NBCV, npf.all.clean$class2))
}


npf_results[nrow(npf_results)+1,] <- c("Naive Bayes with PCA", 
                                       round(accuracy(phat_NB_train, class2_num_train),3), 
                                       round(accuracy(phat_NB, class2_num),3), 
                                       round(mean(l.acc.NBCV),3), 
                                       round(perplexity(phat_NB_train, npf_train$class2),3), 
                                       round(perplexity(phat_NB, npf_valid$class2),3), 
                                       round(mean(l.perp.NBCV),3))

```

3) Logistic regression

To compare the results with logistic regression with Lasso we train also logistic regression on the training data with all the 26 variables. We use scaled data to avoid dominance of variables measured in high magnitudes.

```{r, message=FALSE, echo=FALSE,warning=FALSE}

phat_LG <- function(npf_train.scaled,
  npf_valid.scaled,
  model = function(data) glm(class2 ~ ., data, family = "binomial"),
  pred = function(model, x) predict(model, newdata = x, type = "response")) {
    m <- model(npf_train.scaled)
    pred(m, npf_valid.scaled)
}

npf_train.scaled <- data.frame(scale(npf_train[,-1]),class2=npf_train$class2)
npf_valid.scaled <- data.frame(scale(npf_valid[,-1]),class2=npf_valid$class2)

#Train
phat_LG_train <- phat_LG(npf_train.scaled, npf_train.scaled)

#Test
phat_LG <- phat_LG(npf_train.scaled, npf_valid.scaled)


npf.all.clean.scaled <- data.frame(scale(npf.all.clean[,-1]),class2=npf.all.clean$class2)

#Cross-Validation
l.acc.LGCV = NULL
l.perp.LGCV = NULL
for (l in seq(CV.rep)){
  split = split.CV(npf.all.clean.scaled,10)
  phat_LGCV <- c()
  for (i in 1:10){   
    ind.test = split[[i]]  
    ind.all = unlist(split)   
    ind.train = ind.all[!ind.all %in% ind.test]
    mod <- glm(class2 ~ ., npf.all.clean.scaled[ind.train, ], family = "binomial")
    phat_LGCV[ind.test]<- predict(mod, newdata = npf.all.clean.scaled[ind.test,], type = "response")
  }
  l.acc.LGCV = c(l.acc.LGCV,accuracy(phat_LGCV, class2_num_all))
  l.perp.LGCV = c(l.perp.LGCV,perplexity(phat_LGCV, npf.all.clean$class2))
}

npf_results[nrow(npf_results)+1,] <- c("Logistic regression", 
                                       round(accuracy(phat_LG_train, class2_num_train),3), 
                                       round(accuracy(phat_LG, class2_num),3), 
                                       round(mean(l.acc.LGCV),3), 
                                       round(perplexity(phat_LG_train, npf_train$class2),3), 
                                       round(perplexity(phat_LG, npf_valid$class2),3), 
                                       round(mean(l.perp.LGCV),3))
```

4) k Nearest Neighbor (k-NN)

In k-NN the closest k training data points to the observation are selected and the observation is labeled with the class which occurs most often within the k closest training data points (that is, has the largest proportion). Because the distance is used the data needs to be scaled. Parameter k controls the flexibility of the classifier. The smaller the k the more flexible the decision boundary is. With a very small k the classifier can classify the training data more accurately and decrease the training error but this may end up to overfitting.

To choose an optimal k different values of k are tried. Accuracy on the validation set for each of the selected k are calculated and are respectively 


```{r, message=FALSE, warning=FALSE,echo=FALSE,eval=T}

set.seed(42)

knn.acc <- c()

train.X <- scale(npf_train[,-1])
valid.X <- scale(npf_valid[,-1])
train.Y <- npf_train$class2
valid.Y <- npf_valid$class2
j=0

for (i in c(1, 5, 10, 15, 20, 50)){
  j=j+1
  knn.Y <- knn(train.X, valid.X, train.Y, k=i)
  knn.Y_num <- ifelse(knn.Y=="event",1,0)
  knn.acc[j] <- round(mean(ifelse(knn.Y_num==class2_num,1,0)),3)
}

k<-c("1", "5", "10", "15","20","50")
knn.acc.table <- cbind(k, knn.acc)
colnames(knn.acc.table)<-c("k", "accuracy")
kable(knn.acc.table)
```

When k is 5 or 10, the accuracy is highest. To avoid overfitting we select k = 10 to be one of the methods to compare with other methods.

```{r, message=FALSE, warning=FALSE,echo=FALSE,eval=T}

set.seed(42)
k_sel <- c(10)
train.X <- scale(npf_train[,-1])
valid.X <- scale(npf_valid[,-1])
train.Y <- npf_train$class2
valid.Y <- npf_valid$class2

#Train
knn.Y_train <- knn(train.X, train.X, train.Y, k=k_sel,prob = T)
p.knn_train = attributes(knn.Y_train)$prob
ind.no_train = which(knn.Y_train=="nonevent")
p.knn_train[ind.no_train] = 1-p.knn_train[ind.no_train]

knn.Y_train_num <- ifelse(knn.Y_train=="event",1,0)
knn.acc_train <- round(mean(ifelse(knn.Y_train_num==class2_num_train,1,0)),3)
#the accuracy above is not the same as this:
#accuracy(p.knn_train,ifelse(train.Y=="event",1,0))
knn.perp_train <- round(perplexity(p.knn_train,train.Y),3)


#Validation
knn.Y <- knn(train.X, valid.X, train.Y, k=k_sel,prob = T)
p.knn = attributes(knn.Y)$prob
ind.no_valid = which(knn.Y=="nonevent")
p.knn[ind.no_valid] = 1-p.knn[ind.no_valid]

knn.Y_num <- ifelse(knn.Y=="event",1,0)
knn.acc_valid <- round(mean(ifelse(knn.Y_num==class2_num,1,0)),3)
knn.perp_valid <- round(perplexity(p.knn,valid.Y),3)

#Cross-Validation predictions of the class
l.acc.LGCV = NULL
l.perp.LGCV = NULL
for (l in seq(CV.rep)){
  split = split.CV(npf.all.clean,10)
  knn.Y_CV <- c()
  for (i in 1:10){  
    ind.test = split[[i]] 
    ind.all = unlist(split)  
    ind.train = ind.all[!ind.all %in% ind.test]
    train.X <- scale(npf.all.clean[ind.train,-1])
    valid.X <- scale(npf.all.clean[ind.test,-1])
    train.Y <- npf.all.clean$class2[ind.train]
    valid.Y <- npf.all.clean$class2[ind.test]
    
    knn.Y_CV <- knn(train.X, valid.X, train.Y, k=k_sel,prob = T)
    p.knn= attributes(knn.Y_CV)$prob
    ind.noevent= which(knn.Y_CV=="nonevent")
    p.knn[ind.noevent] = 1-p.knn[ind.noevent]
    knn.acc_CV = accuracy(p.knn,ifelse(valid.Y=="event",1,0))
    knn.perp_CV = perplexity(p.knn,valid.Y)
    # knn.acc_CV <- mean(knn.Y_CV == valid.Y)
  }
  l.acc.LGCV = c(l.acc.LGCV,knn.acc_CV)
  l.perp.LGCV= c(l.perp.LGCV,knn.perp_CV)
}

npf_results[nrow(npf_results)+1,] <- c("10-NN", 
                                       knn.acc_train, 
                                       knn.acc_valid,
                                       round(mean(l.acc.LGCV),3),
                                       knn.perp_train,
                                       knn.perp_valid,
                                       round(mean(l.perp.LGCV),3))
```

Performance measures for the methods are:
```{r, message=FALSE, echo=FALSE,warning=FALSE}
kable(npf_results)
```

As we already concluded logistic regression with Lasso performs better than random forest. Logistic regression with all 26 variables performs surprisingly well. However, perplexity is better when variable selection is done with Lasso. 

Naive Bayes with all 26 variables performs poorly producing high perplexities. When the dimensionality is reduced by using PCA the performance improves significantly but does not outperform logistic regression models. 10-NN performance near the performance of Naive Bayes with PCA.

As expected, the performance of dummy model is worst and is used only as a reference. 
--- This text needs to be updated after the codes are ready!!!

According to the investigated performance measures logistic regression with Lasso is selected as a classifier.


## Summary

We started by looking at the data and especially the correlations between variables. Correlated variables were removed from the data to improve the predictivity of our models. We also noticed that the variables are measured in different units and scales. To avoid the variables with high magnitudes to dominate we used normalized and scaled data when appropriate.

According to the performance comparison we select logistic regression with Lasso as our classifier. It is used to normalized and scaled data - if the scaling is not done the model will predict only events for testing data. The selected model gives the highest accuracy and smallest perplexity in cross-validation. Pros of this method are that the variable selection is done by Lasso and the fact that logistic regression doesn’t make any assumptions of the distribution of the observations in the different classes. We also have enough data compared to the number of variables in cleaned data to train the model properly. By using Lasso and reduced number of variables we also reduce the possibility of overfitting the model. The case of overfitting is investigated by using cross-validation to estimate the generalization error. One of the disadvantages of the model is the assumption of linear decision surface which is not usually valid. However, the method outperformed for example random forest which can handle more complex relationships. After cleaning the data from correlated variables and using Lasso together with logistic regression we can see from the previous result table that the performance measures of the model are pretty good.

--- should we repeat the performance of the selected model here?

## Challenge 11.12.2022

After selecting the model we trained it using the original, cleaned training data (npf_train.csv). The estimates of the probabilities of the observations in testing data (npf_hidden.csv) to be classified as "event" were predicted. If the probability is more than 0.5 the observation is labeled as "event" and otherwise "nonevent". The type of the event was set to be the same as the type that occure mostly in the training data. In the table below shows the frequency of event types in the data when the event has happened. The type "II" has the highest frequency of 48,7 %.

```{r, message=FALSE, echo=FALSE,warning=T, eval = FALSE}

# The frequency of the events classes in the training data, needs npf.var table
events_classes <- c("Ia", "Ib", "II")
events_freq <- c()
for (i in 1:length(events_classes)){
  events_freq[i] <- length(npf.var$class4[npf.var$class4==events_classes[i]])/length(npf.var$class4[npf.var$class4!="nonevent"])  
}
names(events_freq) <- events_classes
events_freq <- as.data.frame(events_freq)
colnames(events_freq) <- c("frequency")
kable(events_freq, digits =3)

```
The accuracy to be added at the beginning of the answers.csv

```{r, message=FALSE, echo=FALSE,warning=T, eval = FALSE}

x.ans <- scale(as.matrix(npf.clean))
y.ans <- npf.all.clean$class2
x.ans.test <- scale(as.matrix(npf_hidden_clean[,-1]))

cv.lambda.ans <- cv.glmnet(x.ans, y.ans, family = "binomial", alpha = 1)$lambda.min
answer.model <- glmnet(x.ans, y.ans, family = "binomial", alpha = 1, lambda = cv.lambda.ans)

p <- predict(answer.model, newx = x.ans.test, type = "response")
class4 <- ifelse(p>0.5,"II", "nonevent")

answers <- data.frame(class4, p)
colnames(answers) <- c("class4", "p")

write.csv(answers, "C:\\Users\\sarir\\OneDrive\\Työpöytä\\Machine Learning OPISKELU\\HY\\Term project\\IML2022\\answers.csv", row.names = FALSE)

#I just copy-pasted the accuracy at the beginning of the CSV file after exporting it...
mean(l.acc.LGLCV)

#For the final CSV we must run the lint.R (code to use in the terminal: Rscript -- vanilla lint.R answers.csv)
```

## Annex: Histograms of variables

After removing variables with high correlation to other variables 26 variables are left in the data. The histograms give an overview of the values of the variables:

```{r, message=FALSE,warning=FALSE, echo=FALSE,fig.show="hold", out.width="30%"}
par(mar = c(4, 4, .1, .1))

for (i in colnames(npf_train[,-1])){
  
    # hist(npf_train[,i],main = i,col = npf_train$class2)
  plot(ggplot(npf_train,aes(x=npf_train[,i],fill=npf_train$class2,color=npf_train$class2))+
    geom_histogram(position="identity",alpha=0.5)+
    xlab(i)+
    theme_bw()+
    theme(panel.grid.minor = element_blank(),plot.title = element_text(hjust = 0.5),
        legend.text=element_text(size=11),legend.text.align = 0)+ 
    theme(legend.position = "top")+
      labs(fill="Class2:",color="Class2:"))
        # ggtitle(i))
      
  #boxplot(pf_train[,i] ~class2, npf_train)
}# which training set ? 
npf.all.clean = cbind(class2=npf.var$class2,npf.clean) # data without correlated for class2
# npf = cbind(class2=npf.var$class4,npf.clean) # data without correlated for class4
# npf.all.full = npf.var[,-1] # Raw data for class2
# npf = npf.var[,-2] # Raw data for class4

set.seed(42)
train_id <- sample(1:nrow(npf), 232)
npf_train <- npf.all.clean[train_id,]
npf_valid <- npf.all.clean[-train_id,]


```


## END

## Some testing and old codes (keep or remove?)

## Testing the CV code

if perplexity should be calculated in each iteration in CV and take an average of the 10 results. Test is done to logistic regression on selected data. CV Perplexity in the result table is 1.274 and now the result is
```{r, message=FALSE, warning=FALSE,echo=FALSE,eval=TRUE,eval=FALSE}

split <- c(rep_len(1:10, length.out = nrow(npf_trimmed)))

perplexity_testi <- c()

for (i in 1:10){
  phat_testi <- c()
  mod <- glm(class2 ~ ., npf_trimmed[ind.train, ], family = "binomial")
  phat_testi<- predict(mod, newdata = npf_trimmed[ind.test,], type = "response")
  perplexity_testi[i] <- perplexity(phat_testi, npf_trimmed$class2[ind.test])
}

sum(perplexity_testi)/10

```

## Should we remove this old chapter: Conclusions and feature selection

--- !!! This whole part can be removed if it's not needed. The text is not updated after cleaning the data so at least the conclusions should be updated. The models in the next session is set to use cleaned data instead of the selection previously done here. !!!

The accuracy is best for Random Forest, but also Logistic Regression with Lasso where lambda is selected using CV ("log reg CV") performs very well.
---- Should we investigate diagnostic plots for log reg? Should be give weight to well performed classifiers when selecting the variables?

All models use the following variables 
- RHIRGA: mean is more important than std according to tree and RF
- H2O mean
- O3 mean (Logistic Regression with Lasso, lambda = CV ("log reg CV") gives small value to std)
- CO2: tree and RF uses only std, log regs give value to mean too
- T std
- SWS.mean 

The following variables are used in all other models but RF
- NO.std
- Pamb0.mean or .std

The following variables are used in all other models but "normal" tree
- CS.mean (.std only in log reg CV)

These variables are used in some models
- UV_B.std is used only on log reg CV 
- UV_A.mean is used only in RF
- RGlob.mean and RGlob.std (RF, mean also in log reg 0.1)
- PTG.std (tree, RF)
- NET.mean
- RPAR.mean
- SO2

Because the measures in different heights/levels are highly correlated, we select just one of them.
---- which ones? Should we put more weight to the selections in random forest and log reg CV?
---- Should be produce boxplots, histograms and/or scatterplots to selected variables

We select the following variables to be used in models:
---- selected for testing the same variables than in Log Reg Lasso CV but only once if there are multiple values with different heights ------------
```{r, message=FALSE,warning=FALSE, echo=FALSE}
#selection <- c("class2", "CO2168.std", "CO242.mean", "H2O42.mean", "NO168.std", "O3504.mean", "O3168.std", "Pamb0.std", "RHIRGA504.mean", "RHIRGA672.std", "SO2168.std", "SWS.mean", "T672.std", "UV_B.std", "CS.mean", "CS.std")
  #c("class2", "CO2168.mean","CO2168.std", "Glob.mean", "H2O168.mean", "H2O168.std", "NO168.std", "O3168.mean", "Pamb0.mean", "Pamb0.std", "PTG.std", "RGlob.mean", "RHIRGA168.mean", "RHIRGA168.std", "SWS.mean", "T168.std", "CS.mean", "CS.std")
#npf_train_trimmed <- npf_train[, colnames(npf_train) %in% selection]
#npf_valid_trimmed <- npf_valid[, colnames(npf_train) %in% selection]
#npf_trimmed <- npf[, colnames(npf) %in% selection]
#selection
```

Let's check if there are any correlation left:

```{r, message=FALSE, warning=FALSE,echo=FALSE}
#cm <- cor(npf_trimmed[,-16])
#colnames(cm) <- rownames(cm) <- sapply(colnames(cm), function(s) gsub(".mean","",s))
#corrplot(cm, order = "FPC", tl.cex=0.5, tl.col= "black")
```

## Do we remove this: A basic decision tree

2) A classification tree selects the following variables with the misclassification as follows

```{r, message=FALSE,warning=FALSE, echo=FALSE, eval = FALSE}

set.seed(42)
tree.npf <- tree(class2~., npf_train)
summary(tree.npf) 
kable(summary(tree.npf)$used,col.names = "Variables")
```

```{r, message=FALSE,warning=FALSE, echo=FALSE, eval=FALSE}
plot(tree.npf)
text(tree.npf, pretty = 0, cex=0.6)
```
Accuracy of the tree can be calculated from the confusion matrix. Predicted classes vs. actual classes for validation data:

```{r, message=FALSE,warning=FALSE, echo=FALSE, eval = FALSE}
tree.pred <- predict(tree.npf, npf_valid, type = "class")
kable(table(tree.pred, npf_valid$class2))
```
Predicted classes vs. actual classes for training data:
--- this can actually be seen from the results above (summary(tree.npf)) - do we want to leave this confusion matrix away?

```{r, message=FALSE,warning=FALSE, echo=FALSE, eval = FALSE}
tree.pred_train <- predict(tree.npf, npf_train, type = "class")
kable(table(tree.pred_train, npf_train$class2))
```
The accuracies are respectively
```{r, message=FALSE,warning=FALSE, echo=FALSE, eval = FALSE}
acc_tree <- (113+107)/232
acc_tree_train <- (104+94)/232
npf_results[nrow(npf_results)+1,] <- c("Tree", round(acc_tree_train,3), round(acc_tree,3), "","","","")
round(acc_tree,3)
round(acc_tree_train,3)
```

## Do we need the CV-code from problem 2?

```{r, message=FALSE, warning=FALSE,echo=FALSE, eval = FALSE}
#this is CV function from Problem 2 is below but I have not used it since both NB and logistic regression needs some modifications...
#Cross-Validation Predictions
#1) split n items into k fold of roughly equal size
kpart <- function(n,k){rep_len(1:k, lenght.out=n)}
#2) find cross-validation predictions
cv <- function(
    formula,
    data,
    model = lm,
    n = nrow(data),
    k = min(n, 10),
    split = kpart(n,k),
    train = function(data){model(formula, data = data)},
    pred = function(model, data){predict(model, newdata = data)}){
  phat <- NULL
  for (i in 1:k){
    mod <- train(data[ind.train, ])
    if (is.null(phat)){
      phat <- pred(mod, data)
    } else {
      phat[ind.test]<- pred(mod, data[ind.test,])
    }
  }
  phat
}

```
